<!-- P2M_REPORT -->
<!-- GENERATED at 2026-01-05 10:11:27 -->
# Project Export: backend

## Overview

- Root: `/home/skinner/rag_chat_bot/backend`
- Files: **30**
- Total size: **133818 bytes**
- Total LOC: 4170 | SLOC: 3149 | TODOs: 1

### Language mix
- python: 24
- plain: 3
- toml: 1
- ini: 1
- dockerfile: 1

### Detected dependencies (best-effort)
- **python_requirements** (15):
  - fastapi==0.124.0
  - uvicorn[standard]==0.38.0
  - SQLAlchemy==2.0.44
  - psycopg[binary]==3.2.13
  - python-dotenv==1.2.1
  - python-jose==3.5.0
  - passlib==1.7.4
  - email-validator==2.3.0
  - bcrypt==4.3.0
  - alembic==1.13.3
  - psycopg2-binary==2.9.10
  - pydantic==2.9.2
  - pydantic-settings==2.6.1
  - chromadb==1.3.5
  - requests==2.32.5
- **pyproject_toml_preview** (4):
  - target-version=py312
  - line-length=100
extend-exclude = [
  - select=[
  - ignore=[
  

### Top 12 largest files (bytes)
- `app/routers/rag.py` — 49405 bytes
- `app/rag_store.py` — 26297 bytes
- `app/weather_service.py` — 10126 bytes
- `tests/test_rag_api.py` — 9577 bytes
- `tests/test_rag_embed_and_chunk_jp.py` — 4740 bytes
- `tests/test_rag_store_collection_ops.py` — 4224 bytes
- `tests/test_rag_store.py` — 3345 bytes
- `tests/test_rag_store_ingestion_json.py` — 3011 bytes
- `tests/test_main_lifespan.py` — 2820 bytes
- `tests/test_rag_ollama_helpers.py` — 2434 bytes
- `app/routers/auth.py` — 2209 bytes
- `app/main.py` — 2134 bytes

### Top 12 longest files (LOC)
- `app/routers/rag.py` — 1478 LOC
- `app/rag_store.py` — 866 LOC
- `app/weather_service.py` — 302 LOC
- `tests/test_rag_api.py` — 274 LOC
- `tests/test_rag_embed_and_chunk_jp.py` — 145 LOC
- `tests/test_rag_store_collection_ops.py` — 130 LOC
- `tests/test_rag_store.py` — 113 LOC
- `tests/test_rag_store_ingestion_json.py` — 92 LOC
- `tests/test_main_lifespan.py` — 86 LOC
- `tests/test_rag_ollama_helpers.py` — 79 LOC
- `app/main.py` — 76 LOC
- `tests/test_auth.py` — 60 LOC

### Project tree (included subset)
```
backend/
├── app/
│   ├── routers/
│   │   ├── auth.py
│   │   └── rag.py
│   ├── database.py
│   ├── main.py
│   ├── models.py
│   ├── rag_store.py
│   ├── schemas.py
│   ├── security.py
│   └── weather_service.py
├── tests/
│   ├── conftest.py
│   ├── test_auth.py
│   ├── test_auth_extra.py
│   ├── test_chunk_text.py
│   ├── test_health.py
│   ├── test_main_lifespan.py
│   ├── test_rag_api.py
│   ├── test_rag_embed_and_chunk_jp.py
│   ├── test_rag_ollama_helpers.py
│   ├── test_rag_store.py
│   ├── test_rag_store_collection_ops.py
│   ├── test_rag_store_config.py
│   ├── test_rag_store_ingestion_json.py
│   ├── test_schemas_security_underscore.py
│   └── test_smoke.py
├── Dockerfile
├── Dockerfile.test
├── pyproject.toml
├── pytest.ini
├── requirements.test.txt
└── requirements.txt
```

## Table of contents (files)

- 1. [app/database.py](#app-database.py)
- 2. [app/main.py](#app-main.py)
- 3. [app/models.py](#app-models.py)
- 4. [app/rag_store.py](#app-rag_store.py)
- 5. [app/routers/auth.py](#app-routers-auth.py)
- 6. [app/routers/rag.py](#app-routers-rag.py)
- 7. [app/schemas.py](#app-schemas.py)
- 8. [app/security.py](#app-security.py)
- 9. [app/weather_service.py](#app-weather_service.py)
- 10. [Dockerfile](#Dockerfile)
- 11. [Dockerfile.test](#Dockerfile.test)
- 12. [pyproject.toml](#pyproject.toml)
- 13. [pytest.ini](#pytest.ini)
- 14. [requirements.test.txt](#requirements.test.txt)
- 15. [requirements.txt](#requirements.txt)
- 16. [tests/conftest.py](#tests-conftest.py)
- 17. [tests/test_auth.py](#tests-test_auth.py)
- 18. [tests/test_auth_extra.py](#tests-test_auth_extra.py)
- 19. [tests/test_chunk_text.py](#tests-test_chunk_text.py)
- 20. [tests/test_health.py](#tests-test_health.py)
- 21. [tests/test_main_lifespan.py](#tests-test_main_lifespan.py)
- 22. [tests/test_rag_api.py](#tests-test_rag_api.py)
- 23. [tests/test_rag_embed_and_chunk_jp.py](#tests-test_rag_embed_and_chunk_jp.py)
- 24. [tests/test_rag_ollama_helpers.py](#tests-test_rag_ollama_helpers.py)
- 25. [tests/test_rag_store.py](#tests-test_rag_store.py)
- 26. [tests/test_rag_store_collection_ops.py](#tests-test_rag_store_collection_ops.py)
- 27. [tests/test_rag_store_config.py](#tests-test_rag_store_config.py)
- 28. [tests/test_rag_store_ingestion_json.py](#tests-test_rag_store_ingestion_json.py)
- 29. [tests/test_schemas_security_underscore.py](#tests-test_schemas_security_underscore.py)
- 30. [tests/test_smoke.py](#tests-test_smoke.py)

---

## Files

<a id="app-database.py"></a>
### 1. `app/database.py`
- Size: 687 bytes | LOC: 31 | SLOC: 25 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 7a20f864e7ab | Py: funcs=1                       classes=0                       complexity≈2

#### Brief
import os

#### Auto Summary
Python module with 1 functions and 0 classes.

#### Content

```python
import os

from sqlalchemy import create_engine
from sqlalchemy.engine import URL
from sqlalchemy.orm import sessionmaker

DB_USER = os.getenv("DB_USER")
DB_PASS = os.getenv("DB_PASS")
DB_HOST = os.getenv("DB_HOST", "db")
DB_PORT = int(os.getenv("DB_PORT", "5432"))
DB_NAME = os.getenv("DB_NAME")

DATABASE_URL = URL.create(
    "postgresql+psycopg",
    username=DB_USER,
    password=DB_PASS,
    host=DB_HOST,
    port=DB_PORT,
    database=DB_NAME,
)

engine = create_engine(DATABASE_URL, pool_pre_ping=True)
SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)


def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

<a id="app-main.py"></a>
### 2. `app/main.py`
- Size: 2134 bytes | LOC: 76 | SLOC: 55 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: e22bfc26b09f | Py: funcs=3                       classes=0                       complexity≈10

#### Brief
backend/app/main.py

#### Auto Summary
Python module with 3 functions and 0 classes.

#### Content

```python
# backend/app/main.py
import logging
import os
from contextlib import asynccontextmanager
from pathlib import Path

import rag_store
from database import engine
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from models import Base
from routers import auth, rag
from sqlalchemy import text

logger = logging.getLogger(__name__)

def _truthy(value: str | None) -> bool:
    if value is None:
        return False
    return value.strip().lower() in {"1", "true", "yes", "y", "on"}

@asynccontextmanager
async def lifespan(app: FastAPI):
    # DB tables
    Base.metadata.create_all(bind=engine)

    # Optional: build the vector DB from local JSON files at startup
    if _truthy(os.getenv("RAG_AUTO_INDEX", "false")):
        docs_dir = os.getenv("RAG_DOCS_DIR") or os.getenv("DOCS_DIR", "/data/json")
        rebuild = _truthy(os.getenv("RAG_REBUILD_ON_START", "true"))
        fail_fast = _truthy(os.getenv("RAG_FAIL_FAST") or os.getenv("RAG_INDEX_FAIL_FAST", "false"))


        try:
            if rebuild:
                stats = rag_store.rebuild_from_json_dir(docs_dir)
            else:
                stats = rag_store.ingest_json_dir(docs_dir)
            logger.info("RAG index ready: %s", stats)
        except Exception as exc:
            logger.exception("RAG auto-index failed: %s", exc)
            if fail_fast:
                raise

    yield


def create_app() -> FastAPI:
    app = FastAPI(lifespan=lifespan)

    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    app.include_router(auth.router)
    app.include_router(rag.router)


    # static /public
    public_dir = Path(os.getenv("PUBLIC_DIR", "/public"))
    app.mount("/public", StaticFiles(directory=str(public_dir), check_dir=False), name="public")

    return app

app = create_app()

@app.get("/health")
def health():
    with engine.connect() as conn:
        ok = conn.execute(text("SELECT 1")).scalar() == 1
    return {"status": "ok", "db": ok}
```

<a id="app-models.py"></a>
### 3. `app/models.py`
- Size: 672 bytes | LOC: 21 | SLOC: 15 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 8fdcc3c89e61 | Py: funcs=0                       classes=3                       complexity≈1

#### Brief
from sqlalchemy import Integer, String
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column

#### Auto Summary
Python module with 0 functions and 3 classes.

#### Content

```python
from sqlalchemy import Integer, String
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column


class Base(DeclarativeBase):
    pass


class Item(Base):
    __tablename__ = "items"
    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)
    title: Mapped[str] = mapped_column(String(120), nullable=False, index=True)


class User(Base):
    __tablename__ = "users"
    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)
    email: Mapped[str] = mapped_column(
        String(255), unique=True, index=True, nullable=False
    )
    hashed_password: Mapped[str] = mapped_column(String(255), nullable=False)
```

<a id="app-rag_store.py"></a>
### 4. `app/rag_store.py`
- Size: 26297 bytes | LOC: 866 | SLOC: 644 | TODOs: 0 | Modified: 2026-01-02 16:35:31 | SHA1: e1bb43855472 | Py: funcs=31                       classes=2                       complexity≈213

#### Brief
import json
import logging

#### Auto Summary
Python module with 32 functions and 0 classes.

#### Content

```python
import json
import logging
import os
import re
import uuid
import time
from pathlib import Path
from typing import Any

import chromadb
import requests

logger = logging.getLogger(__name__)

# -------------------------------------------------------------------
# Simple data containers
# -------------------------------------------------------------------
def _slug(s: str) -> str:
    s = re.sub(r"[^0-9A-Za-z_]+", "_", s).strip("_").lower()
    return s or "tag"

def _chroma_safe_metadata(meta: dict[str, Any]) -> dict[str, Any]:
    """
    Chroma metadata: list/dict
    - list[str] -> "a,b,c"
    - dict/list -> JSON
    """
    out: dict[str, Any] = {}
    for k, v in (meta or {}).items():
        key = str(k)

        if v is None or isinstance(v, (str, int, float, bool)):
            out[key] = v
            continue

        if isinstance(v, list):
            if all(isinstance(x, str) for x in v):
                out[key] = ",".join(v)
                if key in {"tags", "tag"}:
                    for t in v:
                        out[f"tag__{_slug(t)}"] = True
            else:
                out[key] = json.dumps(v, ensure_ascii=False)
            continue

        if isinstance(v, dict):
            out[key] = json.dumps(v, ensure_ascii=False)
            continue

        out[key] = str(v)

    return out

class DocumentChunk:
    """Internal representation of a chunked piece of text."""

    def __init__(self, text, metadata=None):
        self.text = text
        self.metadata = metadata or {}


class RAGChunk:
    """Chunk plus distance from query, used as RAG context."""

    def __init__(self, text, distance, metadata=None):
        self.text = text
        self.distance = distance
        self.metadata = metadata or {}


# -------------------------------------------------------------------
# Configuration
# -------------------------------------------------------------------

_CHROMA_DB_DIR_ENV = "CHROMA_DB_DIR"
_CHROMA_COLLECTION_ENV = "CHROMA_COLLECTION_NAME"
_DEFAULT_CHROMA_DB_DIR = "/chroma"
_DEFAULT_COLLECTION_NAME = "documents"

_OLLAMA_BASE_URL_ENV = "OLLAMA_BASE_URL"
_OLLAMA_EMBED_MODEL_ENV = "EMBEDDING_MODEL"
_DEFAULT_OLLAMA_BASE_URL = "http://ollama:11434"
_DEFAULT_EMBED_MODEL = "nomic-embed-text"

_RAG_CHUNK_SIZE_ENV = "RAG_CHUNK_SIZE"

# Lint-friendly constants
_HTTP_STATUS_NOT_FOUND = 404
_JSON_STRING_CONTROL_CHAR_MAX_EXCLUSIVE = 0x20
_OLLAMA_REQUEST_TIMEOUT_SECONDS = 30
_OLLAMA_ROUTE_NOT_FOUND_MARKER = "404 page not found"


_client = None
_collection = None


# -------------------------------------------------------------------
# Helpers to read environment
# -------------------------------------------------------------------


def _get_chroma_db_dir():
    """Return the directory where Chroma will store data."""
    value = os.getenv(_CHROMA_DB_DIR_ENV)
    if value:
        return value
    return _DEFAULT_CHROMA_DB_DIR


def _get_chroma_collection_name():
    """Return the Chroma collection name."""
    value = os.getenv(_CHROMA_COLLECTION_ENV)
    if value:
        return value
    return _DEFAULT_COLLECTION_NAME


def _get_ollama_base_url():
    """Return base URL for Ollama HTTP API, e.g. http://ollama:11434"""
    value = os.getenv(_OLLAMA_BASE_URL_ENV)
    if value:
        return value
    return _DEFAULT_OLLAMA_BASE_URL


def _get_embedding_model():
    """Return embedding model name, e.g. `nomic-embed-text`."""
    value = os.getenv(_OLLAMA_EMBED_MODEL_ENV)
    if value:
        return value
    return _DEFAULT_EMBED_MODEL


# -------------------------------------------------------------------
# Chroma client / collection singletons
# -------------------------------------------------------------------


def _get_chroma_client():
    """
    Return a module-level singleton Chroma client.

    Tests expect:
      * The path is taken from CHROMA_DB_DIR (or default).
      * The same instance is returned when called twice.
    """

    if _client is not None:
        return _client

    db_dir = _get_chroma_db_dir()
    logger.info("Creating Chroma client at %s", db_dir)
    client = chromadb.PersistentClient(path=db_dir)
    # keep module-level cache without using `global`
    globals()["_client"] = client
    return client


def _get_collection():
    """
    Return a module-level singleton Chroma collection.

    Tests monkeypatch `_collection` directly, so if `_collection` is not
    None we just give it back and do NOT create a new one.
    """
    if _collection is not None:
        return _collection

    client = _get_chroma_client()
    name = _get_chroma_collection_name()
    logger.info("Getting/creating Chroma collection %s", name)
    collection = client.get_or_create_collection(name=name)
    # store on module without `global`
    globals()["_collection"] = collection
    return collection


# -------------------------------------------------------------------
# Chunking
# -------------------------------------------------------------------

_DEFAULT_CHUNK_SIZE=128


def _get_chunk_size() -> int:
    """
    Read chunk size from env (RAG_CHUNK_SIZE). Falls back to _DEFAULT_CHUNK_SIZE.
    """
    raw = os.getenv(_RAG_CHUNK_SIZE_ENV, "").strip()
    if not raw:
        return _DEFAULT_CHUNK_SIZE
    try:
        v = int(raw)
        return v if v > 0 else _DEFAULT_CHUNK_SIZE
    except ValueError:
        logger.warning(
            "Invalid %s=%r; falling back to default=%d",
            _RAG_CHUNK_SIZE_ENV,
            raw,
            _DEFAULT_CHUNK_SIZE,
        )
        return _DEFAULT_CHUNK_SIZE

JP_SENT_SPLIT = re.compile(r"(?<=[。！？])")  # noqa: RUF001
CJK_RE = re.compile(r"[\u3040-\u30ff\u4e00-\u9fff]")

def chunk_text(text: str, max_tokens: int = _DEFAULT_CHUNK_SIZE) -> list[DocumentChunk]:
    """Split text into chunks with basic sentence-aware behavior.

    - For CJK text (Japanese/Chinese/Korean), split on Japanese punctuation and
      *do not* pack multiple sentences into a single chunk by default. This keeps
      retrieval snippets semantically tight.
    - For non-CJK text, split by whitespace into ~`max_tokens` words.

    Notes:
    - This is a simple heuristic chunker. If you need more accurate token counts,
      integrate a tokenizer (e.g., tiktoken) and chunk by tokens instead of chars.
    """
    if not text:
        return []

    has_cjk = CJK_RE.search(text) is not None
    chunks: list[DocumentChunk] = []

    if has_cjk:
        # JP_SENT_SPLIT uses look-behind so punctuation stays with the sentence.
        sentences = [s.strip() for s in JP_SENT_SPLIT.split(text) if s.strip()]
        for s in sentences:
            if len(s) <= max_tokens:
                chunk_index = len(chunks)
                chunks.append(
                    DocumentChunk(
                        s,
                        {"chunk_index": chunk_index, "index": chunk_index},
                    )
                )
                continue

            # Very long "sentence" (or no punctuation) -> split by characters.
            for i in range(0, len(s), max_tokens):
                part = s[i : i + max_tokens].strip()
                if not part:
                    continue
                chunk_index = len(chunks)
                chunks.append(
                    DocumentChunk(
                        part,
                        {"chunk_index": chunk_index, "index": chunk_index},
                    )
                )

    else:
        words = text.split()
        current: list[str] = []
        count = 0

        for w in words:
            if count >= max_tokens and current:
                chunk_index = len(chunks)
                chunk_text_value = " ".join(current)
                chunks.append(
                    DocumentChunk(
                        chunk_text_value,
                        {"chunk_index": chunk_index, "index": chunk_index},
                    )
                )
                current = []
                count = 0

            current.append(w)
            count += 1

        if current:
            chunk_index = len(chunks)
            chunk_text_value = " ".join(current)
            chunks.append(
                DocumentChunk(
                    chunk_text_value,
                    {"chunk_index": chunk_index, "index": chunk_index},
                )
            )

    total = len(chunks)
    for c in chunks:
        c.metadata["total_chunks"] = total

    return chunks

# -------------------------------------------------------------------
# Embeddings (Ollama)
# -------------------------------------------------------------------

def _ollama_embed_attempts(model: str, text: str) -> list[tuple[str, dict[str, Any]]]:
    return [
        ("/api/embed", {"model": model, "input": text}),        # newer
        ("/api/embeddings", {"model": model, "prompt": text}),  # older
    ]


def _safe_response_text(response: requests.Response) -> str:
    try:
        return response.text or ""
    except Exception:
        return ""


def _extract_ollama_error_message(response: requests.Response, fallback: str) -> str:
    try:
        data = response.json()
        if isinstance(data, dict) and isinstance(data.get("error"), str):
            return data["error"]
    except Exception:
        pass
    return fallback


def _should_try_next_endpoint(
    response: requests.Response,
    model: str,
    http_err: requests.HTTPError,
) -> bool:
    status = getattr(response, "status_code", None)
    if status != _HTTP_STATUS_NOT_FOUND:
        return False

    body_text = _safe_response_text(response)
    err_msg = _extract_ollama_error_message(response, body_text)
    lower = (err_msg or "").lower()

    # Ollama: 404 can mean "model does not exist"
    if "model" in lower and ("not found" in lower or "does not exist" in lower):
        raise RuntimeError(
            f"Ollama model is not available: {model!r}. "
            f"Pull it first: `docker compose exec ollama ollama pull {model}`"
        ) from http_err

    # Missing route => try next endpoint
    return _OLLAMA_ROUTE_NOT_FOUND_MARKER in (body_text or "").lower()


def _extract_embedding_from_response(data: Any) -> list[float] | None:
    if not isinstance(data, dict):
        return None

    embeddings = data.get("embeddings")
    if isinstance(embeddings, list) and embeddings and isinstance(embeddings[0], list):
        return embeddings[0]

    embedding = data.get("embedding")
    if isinstance(embedding, list):
        return embedding

    rows = data.get("data")
    if isinstance(rows, list) and rows and isinstance(rows[0], dict):
        emb = rows[0].get("embedding")
        if isinstance(emb, list):
            return emb

    return None


def _embed_with_ollama(text: str) -> list[float]:
    base_url = _get_ollama_base_url().rstrip("/")
    model = _get_embedding_model()

    last_error: Exception | None = None
    for endpoint, payload in _ollama_embed_attempts(model, text):
        url = base_url + endpoint
        try:
            response = requests.post(
                url,
                json=payload,
                timeout=_OLLAMA_REQUEST_TIMEOUT_SECONDS,
            )
            try:
                response.raise_for_status()
            except requests.HTTPError as http_err:
                if _should_try_next_endpoint(response, model, http_err):
                    last_error = http_err
                    continue
                raise

            data = response.json()
            embedding = _extract_embedding_from_response(data)
            if isinstance(embedding, list) and embedding:
                return embedding
            raise ValueError(f"Unexpected embedding response format at {endpoint}")
        except Exception as exc:
            last_error = exc

    if last_error is None:
        raise RuntimeError("Ollama embedding failed")
    raise RuntimeError(f"Ollama embedding failed: {last_error}") from last_error


def embed_texts(texts: list[str]) -> list[list[float]]:
    if not texts:
        return []

    cleaned_texts: list[str] = []
    for t in texts:
        if not t:
            continue
        stripped = t.strip()
        if not stripped:
            continue
        cleaned_texts.append(stripped)

    if not cleaned_texts:
        return []

    embeddings: list[list[float]] = []
    errors: list[Exception] = []

    for t in cleaned_texts:
        last_exc: Exception | None = None
        for attempt in range(3):
            try:
                embeddings.append(_embed_with_ollama(t))
                last_exc = None
                break
            except Exception as exc:
                last_exc = exc
                if attempt < 2:
                    time.sleep(1)

        if last_exc is not None:
            logger.exception("Embedding failed for text chunk", exc_info=last_exc)
            errors.append(last_exc)

    if not embeddings:
        raise RuntimeError(
            f"Failed to embed any of the {len(cleaned_texts)} text chunks; "
            f"last error: {errors[-1] if errors else 'unknown'}"
        )

    if errors:
        raise RuntimeError(
            f"Embedding failed for {len(errors)}/{len(cleaned_texts)} chunks; "
            f"last error: {errors[-1] if errors else 'unknown'}"
        )

    return embeddings


# -------------------------------------------------------------------
# Ingestion
# -------------------------------------------------------------------

def add_document(text: str) -> None:
    """Split text into chunks, embed them, and store in Chroma."""
    chunks = chunk_text(text, max_tokens=_get_chunk_size())
    if not chunks:
        logger.warning("No chunks produced from text; nothing to add.")
        return

    documents = [c.text for c in chunks]
    metadatas = [c.metadata for c in chunks]
    ids = [str(uuid.uuid4()) for _ in chunks]

    embeddings = embed_texts(documents)

    if not embeddings:
        raise RuntimeError("No embeddings produced; document not stored.")

    if len(embeddings) != len(documents):
        raise ValueError(
            f"Embedding count mismatch: {len(embeddings)} vs {len(documents)}"
        )

    collection = _get_collection()
    collection.add(
        documents=documents,
        embeddings=embeddings,
        metadatas=metadatas,
        ids=ids,
    )

# -------------------------------------------------------------------
# Retrieval
# -------------------------------------------------------------------


def query_similar_chunks(question, top_k=3):
    """
    Embed the question, query Chroma, and return a list of RAGChunk.

    Tests expect:

        query_similar_chunks(question: str, top_k: int = 3) -> list[RAGChunk]

    and then they monkeypatch this function in some API tests, but in
    `test_rag_store.py` they call the real one with a DummyCollection that
    implements:

        def query(self,
                  query_embeddings: list[list[float]],
                  n_results: int,
                  include: list[str] | None = None) -> dict[str, Any]

    The expected keys in the response are:

        {
          "documents": [[...]],
          "metadatas": [[...]],
          "distances": [[...]],
        }
    """
    cleaned = (question or "").strip()
    if not cleaned:
        return []

    query_embeddings = embed_texts([cleaned])
    if not query_embeddings:
        # Could happen if embedding fails; in that case, just return empty.
        return []

    collection = _get_collection()
    result = collection.query(
        query_embeddings=query_embeddings,
        n_results=top_k,
        include=["documents", "metadatas", "distances"],
    )

    docs_lists = result.get("documents") or [[]]
    metas_lists = result.get("metadatas") or [[]]
    dists_lists = result.get("distances") or [[]]

    docs = docs_lists[0] if docs_lists else []
    metas = metas_lists[0] if metas_lists else []
    dists = dists_lists[0] if dists_lists else []

    chunks: list[RAGChunk] = []
    for doc, raw_meta, dist in zip(docs, metas, dists, strict=False):
        if isinstance(raw_meta, dict):
            normalized_meta = raw_meta
        elif raw_meta is None:
            normalized_meta = {}
        else:
            # Normalize strange metadata formats used in some backends.
            normalized_meta = {"value": raw_meta}

        chunks.append(
            RAGChunk(
                text=doc,
                distance=dist,
                metadata=normalized_meta,
            )
        )

    return chunks

# -------------------------------------------------------------------
# JSON directory ingestion (no UI / no input form)
# -------------------------------------------------------------------


def list_json_files(docs_dir: str) -> list[str]:
    """Return a sorted list of *.json files under `docs_dir` (non-recursive)."""
    try:
        root = Path(docs_dir)
    except Exception:
        return []

    if not root.exists() or not root.is_dir():
        return []

    return sorted([str(p) for p in root.glob("*.json")])

def _escape_control_chars_inside_json_strings(raw: str) -> str:
    """
    Make 'almost JSON' parseable by escaping control characters (< 0x20)
    that appear *inside* JSON double-quoted strings.

    This fixes hand-edited JSON like:
      "text": "hello
      world"
    which is invalid JSON (literal newline in a string).
    """
    out: list[str] = []
    in_string = False
    escaped = False

    for ch in raw:
        if not in_string:
            out.append(ch)
            if ch == '"':
                in_string = True
                escaped = False
            continue

        # inside a JSON string
        if escaped:
            out.append(ch)
            escaped = False
            continue

        if ch == "\\":
            out.append(ch)
            escaped = True
            continue

        if ch == '"':
            out.append(ch)
            in_string = False
            continue

        code = ord(ch)
        if code < _JSON_STRING_CONTROL_CHAR_MAX_EXCLUSIVE:
            if ch == "\n":
                out.append("\\n")
            elif ch == "\r":
                out.append("\\r")
            elif ch == "\t":
                out.append("\\t")
            else:
                out.append(f"\\u{code:04x}")
        else:
            out.append(ch)

    return "".join(out)


def _load_json_file(path: str) -> list[dict]:
    p = Path(path)

    raw = p.read_text(encoding="utf-8")
    try:
        data = json.loads(raw)
    except json.JSONDecodeError:
        fixed = _escape_control_chars_inside_json_strings(raw)
        logger.warning("Invalid JSON fixed by escaping control characters: %s", path)
        data = json.loads(fixed)

    if isinstance(data, dict):
        data = [data]

    if not isinstance(data, list):
        raise ValueError(f"JSON must be an object or list of objects: {path}")

    docs: list[dict] = []
    for item in data:
        if not isinstance(item, dict):
            continue
        doc_id = item.get("id")
        text = item.get("text")
        if not isinstance(doc_id, str) or not doc_id.strip():
            raise ValueError(f"Missing/invalid 'id' in {path}")
        if not isinstance(text, str) or not text.strip():
            raise ValueError(f"Missing/invalid 'text' for id={doc_id!r} in {path}")
        default_source = p.resolve().as_uri()  # file:///...
        meta = item.get("metadata") if isinstance(item.get("metadata"), dict) else {}

        # Promote top-level fields into metadata so they survive ingestion.
        # Many upstream JSON formats keep important fields as top-level keys (e.g. "title", "datetime", "place", "tag", "link(s)").
        # This loader keeps the nested "metadata" dict *and* merges other top-level keys (except id/text/metadata) into metadata.
        meta = dict(meta)

        for k, v in item.items():
            if k in {"id", "text", "metadata"}:
                continue
            if k not in meta:
                meta[k] = v

        def _clean_str_list(val: Any, *, split_commas: bool = False) -> list[str]:
            if val is None:
                return []
            if isinstance(val, list):
                return [x.strip() for x in val if isinstance(x, str) and x.strip()]
            if isinstance(val, str):
                s = val.strip()
                if not s:
                    return []
                if split_commas and "," in s:
                    return [p.strip() for p in s.split(",") if p.strip()]
                return [s]
            return []

        # Normalize links: accept both `link` and `links` (string or list), and common single-URL keys.
        links: list[str] = []
        for k in ("links", "link", "url", "permalink", "href", "source_url", "sourceUrl"):
            links.extend(_clean_str_list(item.get(k), split_commas=True))
            links.extend(_clean_str_list(meta.get(k), split_commas=True))
        # De-dup while preserving order
        links = list(dict.fromkeys([u for u in links if u]))
        if links:
            meta.setdefault("links", links)
            # Back-compat: if upstream uses singular `link`, keep it in sync when possible.
            meta.setdefault("link", links)

        # Normalize tags: accept `tag` (new) and `tags` (legacy), as list[str] where possible.
        tags: list[str] = []
        for k in ("tags", "tag"):
            tags.extend(_clean_str_list(item.get(k), split_commas=True))
            tags.extend(_clean_str_list(meta.get(k), split_commas=True))
        tags = list(dict.fromkeys([t for t in tags if t]))
        if tags:
            meta.setdefault("tags", tags)
            meta.setdefault("tag", tags)

        docs.append(
            {
                "id": doc_id.strip(),
                "text": text,
                "source": item.get("source") or default_source,
                "file": str(p.name),
                "metadata": meta,
            }
        )

    return docs



def get_collection_count() -> int:
    """Return number of stored records (chunks) in the Chroma collection."""
    try:
        return int(_get_collection().count())
    except Exception:
        return 0


def _delete_by_doc_id(doc_id: str) -> None:
    """Best-effort delete of all chunks for a given doc_id."""
    col = _get_collection()
    try:
        # Chroma collections usually support `where` filtering.
        col.delete(where={"doc_id": doc_id})
    except Exception:
        # Fallback: try deleting by ids if we can infer them later; otherwise ignore.
        return


def upsert_document(
    doc_id: str,
    text: str,
    *,
    source: str | None = None,
    metadata: dict | None = None,
    max_tokens: int | None = None,
) -> int:
    """Chunk + embed + store one document with deterministic chunk IDs.

    IDs are generated as: "{doc_id}:{chunk_index}" so re-indexing updates the same
    logical records.

    Returns:
        Number of chunks stored.
    """
    if not isinstance(doc_id, str) or not doc_id.strip():
        raise ValueError("doc_id must be a non-empty string")
    if not isinstance(text, str) or not text.strip():
        raise ValueError("text must be a non-empty string")

    size = int(max_tokens) if isinstance(max_tokens, int) and max_tokens > 0 else _get_chunk_size()
    chunks = chunk_text(text, max_tokens=size)
    if not chunks:
        return 0

    chunk_texts = [c.text for c in chunks]
    embeddings = embed_texts(chunk_texts)
    if len(embeddings) != len(chunk_texts):
        raise RuntimeError(
            f"Embedding count mismatch for doc_id={doc_id}: "
            f"chunks={len(chunk_texts)} embeddings={len(embeddings)}"
    )

    base_meta = _chroma_safe_metadata(dict(metadata or {}))
    base_meta["doc_id"] = doc_id
    if source:
        base_meta.setdefault("source", source)

    metadatas: list[dict] = []
    ids: list[str] = []
    for c in chunks:
        m = dict(base_meta)
        if isinstance(c.metadata, dict):
            m.update(c.metadata)

        m = _chroma_safe_metadata(m)

        # Ensure deterministic chunk id.
        idx = m.get("chunk_index")
        if idx is None:
            idx = m.get("index", 0)

        metadatas.append(m)
        ids.append(f"{doc_id}:{idx}")

    col = _get_collection()

    # Prefer upsert if available. Otherwise, delete existing doc chunks and add.
    if hasattr(col, "upsert"):
        col.upsert(
            embeddings=embeddings,
            documents=chunk_texts,
            metadatas=metadatas,
            ids=ids,
        )
    else:
        _delete_by_doc_id(doc_id)
        col.add(
            embeddings=embeddings,
            documents=chunk_texts,
            metadatas=metadatas,
            ids=ids,
        )

    return len(chunks)


def ingest_json_dir(docs_dir: str) -> dict[str, int]:
    """Ingest every document found in JSON files under `docs_dir`.

    Expected JSON format (per entry):
      {
        "id": "unique_doc_id",
        "text": "document text ...",
        "source": "optional",
        "metadata": {"optional": "dict"}
      }

    Returns:
        {"documents": <count>, "chunks": <count>, "files": <count>}
    """
    json_files = list_json_files(docs_dir)
    documents = 0
    chunks = 0

    for path in json_files:
        entries = _load_json_file(path)
        for e in entries:
            documents += 1
            meta = dict(e.get("metadata") or {})
            # Preserve the file name for traceability.
            meta.setdefault("file", e.get("file"))
            chunks += upsert_document(
                e["id"],
                e["text"],
                source=e.get("source"),
                metadata=meta,
            )

    return {"documents": documents, "chunks": chunks, "files": len(json_files)}


def reset_collection() -> None:
    """Delete and recreate the Chroma collection (best-effort)."""
    name = _get_chroma_collection_name()
    try:
        _get_chroma_client().delete_collection(name)
    except Exception:
        pass
    globals()["_collection"] = None
    _get_collection()


def rebuild_from_json_dir(docs_dir: str) -> dict[str, int]:
    reset_collection()
    try:
        return ingest_json_dir(docs_dir)
    except Exception:
        reset_collection()
        raise
```

<a id="app-routers-auth.py"></a>
### 5. `app/routers/auth.py`
- Size: 2209 bytes | LOC: 54 | SLOC: 45 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: a5d71be66a14 | Py: funcs=3                       classes=0                       complexity≈10

#### Brief
import models
import schemas

#### Auto Summary
Python module with 3 functions and 0 classes.

#### Content

```python
import models
import schemas
from database import get_db
from fastapi import APIRouter, Depends, Header, HTTPException, status
from security import create_access_token, decode_token, hash_password, verify_password
from sqlalchemy import select
from sqlalchemy.orm import Session

router = APIRouter(prefix="/auth", tags=["auth"])

MIN_LENGTH_PASSWORD = 6


@router.post(
    "/signup", response_model=schemas.UserOut, status_code=status.HTTP_201_CREATED
)
def signup(payload: schemas.UserCreate, db: Session = Depends(get_db)):
    # Explicit password length check to return friendly error
    if len(payload.password) < MIN_LENGTH_PASSWORD:
        raise HTTPException(
            status_code=400, detail="Password must be at least 6 characters."
        )
    exists = db.scalar(select(models.User).where(models.User.email == payload.email))
    if exists:
        raise HTTPException(status_code=400, detail="Email already registered.")
    user = models.User(
        email=payload.email, hashed_password=hash_password(payload.password)
    )
    db.add(user)
    db.commit()
    db.refresh(user)
    return user


@router.post("/signin", response_model=schemas.Token)
def signin(payload: schemas.SignIn, db: Session = Depends(get_db)):
    user = db.scalar(select(models.User).where(models.User.email == payload.email))
    if not user or not verify_password(payload.password, user.hashed_password):
        raise HTTPException(status_code=401, detail="Invalid email or password.")
    token = create_access_token(sub=user.email)
    return {"access_token": token, "token_type": "bearer"}


@router.get("/me", response_model=schemas.UserOut)
def me(authorization: str | None = Header(default=None), db: Session = Depends(get_db)):
    if not authorization or not authorization.lower().startswith("bearer "):
        raise HTTPException(status_code=401, detail="Missing token.")
    email = decode_token(authorization.split(" ", 1)[1])
    if not email:
        raise HTTPException(status_code=401, detail="Invalid token.")
    user = db.scalar(select(models.User).where(models.User.email == email))
    if not user:
        raise HTTPException(status_code=404, detail="User not found.")
    return user
```

<a id="app-routers-rag.py"></a>
### 6. `app/routers/rag.py`
- Size: 49405 bytes | LOC: 1478 | SLOC: 1170 | TODOs: 0 | Modified: 2026-01-05 09:36:38 | SHA1: 976d75b928fb | Py: funcs=53                       classes=8                       complexity≈392

#### Brief
from __future__ import annotations

#### Auto Summary
Python module with 53 functions and 8 classes.

#### Content

```python
from __future__ import annotations

import logging
import os
import re
from http import HTTPStatus
from typing import Any, Literal
import json
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import rag_store
import requests
import hashlib
import random
from fastapi import APIRouter, HTTPException, Request
from pydantic import BaseModel, Field
from rag_store import RAGChunk
from weather_service import *

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/rag", tags=["rag"])

_WEEKDAYS = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]

# Use a module-level session so tests can monkeypatch it.
_session = requests.Session()



def _safe_zoneinfo(tz_name: str) -> ZoneInfo:
    try:
        return ZoneInfo(tz_name)
    except Exception:
        return ZoneInfo("Asia/Tokyo")

def _parse_datetime_like(s: str, tz_name: str) -> datetime | None:
    """
    Parse a datetime-ish string into an aware datetime in tz_name.
    Supports ISO strings and a few loose formats. If tz is missing, assumes tz_name.
    """
    if not isinstance(s, str):
        return None
    raw = s.strip()
    if not raw:
        return None

    # Common suffixes
    cand0 = raw.replace("JST", "+09:00").replace(" UTC", "+00:00")

    # Try ISO (with or without 'T')
    for cand in (cand0, cand0.replace(" ", "T", 1)):
        try:
            dt = datetime.fromisoformat(cand.replace("Z", "+00:00"))
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=_safe_zoneinfo(tz_name))
            else:
                dt = dt.astimezone(_safe_zoneinfo(tz_name))
            return dt
        except Exception:
            pass

    # Fallback: date-only "YYYY-MM-DD"
    m = re.match(r"^\s*(\d{4}-\d{2}-\d{2})", cand0)
    if m:
        try:
            dt = datetime.fromisoformat(m.group(1))
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=_safe_zoneinfo(tz_name))
            return dt
        except Exception:
            return None

    return None

def _resolve_now_datetime(
    live_weather: str | None,
    request_datetime: str | None,
) -> tuple[datetime, str, str]:
    """
    Resolve 'now' with precedence:
      1) request_datetime (authoritative)
      2) live_weather.current.time
      3) server clock in TZ_NAME
    Returns (dt, tz_name, source)
    """
    tz_fallback = os.getenv("TZ_NAME") or "Asia/Tokyo"

    if isinstance(request_datetime, str) and request_datetime.strip():
        dt_req = _parse_datetime_like(request_datetime, tz_fallback)
        if dt_req is not None:
            return dt_req, tz_fallback, "request_datetime"

    dt_w, tz_w = _extract_now_from_live_weather(live_weather)
    if dt_w is not None:
        return dt_w, (tz_w or tz_fallback), "live_weather"

    tz_name = tz_w or tz_fallback
    return datetime.now(_safe_zoneinfo(tz_name)), tz_name, "server_clock"


def _extract_now_from_live_weather(live_weather: str | None) -> tuple[datetime | None, str | None]:
    """
    Try to parse Open-Meteo snapshot JSON:
      { "timezone": "Asia/Tokyo", "current": { "time": "2025-12-28T21:00", ... } }
    """
    if not live_weather or not live_weather.strip():
        return None, None
    try:
        obj = json.loads(live_weather)
        if not isinstance(obj, dict):
            return None, None
        tz_name = obj.get("timezone") or obj.get("timezone_abbreviation") or "Asia/Tokyo"
        cur = obj.get("current") or {}
        t = cur.get("time")
        if not isinstance(t, str) or not t:
            return None, tz_name
        dt = datetime.fromisoformat(t.replace("Z", "+00:00"))
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=_safe_zoneinfo(tz_name))
        else:
            dt = dt.astimezone(_safe_zoneinfo(tz_name))
        return dt, tz_name
    except Exception:
        return None, None

def _format_now_block(live_weather: str | None, request_datetime: str | None = None) -> str:
    dt, tz_name, src = _resolve_now_datetime(live_weather, request_datetime)

    today = dt.date()
    tomorrow = today + timedelta(days=1)

    # to fix "this weekend"
    # weekday(): Mon=0 ... Sun=6
    days_until_sat = (5 - dt.weekday()) % 7
    sat = today + timedelta(days=days_until_sat)
    sun = sat + timedelta(days=1)

    wd = _WEEKDAYS[dt.weekday()]
    return (
        f"- source: {src}\n"
        f"- request_datetime: {request_datetime}\n" if request_datetime else ""
        f"- local_datetime: {dt.strftime('%Y-%m-%d %H:%M')} {dt.tzname() or ''} ({wd})\n"
        f"- timezone: {tz_name}\n"
        f"- today: {today.isoformat()}\n"
        f"- tomorrow: {tomorrow.isoformat()}\n"
        f"- this_weekend: {sat.isoformat()}–{sun.isoformat()} (Sat–Sun)\n"
    )

def _truthy(value: str | None) -> bool:
    if value is None:
        return False
    return value.strip().lower() in {"1", "true", "yes", "y", "on"}


# -------------------------------------------------------------------
# Request / response models
# -------------------------------------------------------------------


class IngestRequest(BaseModel):
    documents: list[str] = Field(..., description="Raw texts to ingest into the vector store.")


class IngestResponse(BaseModel):
    ingested: int


class QueryRequest(BaseModel):
    question: str = Field(..., min_length=1)


    # caller-provided metadata (NOT stored in RAG)
    datetime: str | None = Field(
        default=None,
        description="Optional ISO datetime string provided by the caller (e.g. 2026-01-03T17:43:54+09:00).",
    )
    links: list[str] | None = Field(
        default=None,
        description="Optional list of URLs provided by the caller (for allowlisting / UI display).",
    )

    top_k: int = Field(
        5,
        ge=1,
        le=128,
        description="Number of similar chunks to retrieve from the vector store.",
    )

    extra_context: str | None = Field(
        default=None,
        description=(
            "Optional short-lived context that should NOT be stored in RAG "
            "(e.g., live weather fetched at runtime)."
        ),
    )

    # Backward-compat aliases for older clients/scripts
    context: str | None = Field(default=None, description="Alias for extra_context")
    user_context: str | None = Field(default=None, description="Alias for extra_context")

    include_debug: bool = Field(
        default=False,
        description="If true, include retrieved RAG context and chunk metadata in the response.",
    )

    output_style: Literal["default", "tweet_bot"] = Field(
        default="tweet_bot",
        description="Controls output style. 'tweet_bot' produces a single friendly tweet-like post.",
    )

    max_words: int = Field(
        default=512,
        ge=50,
        le=1024,
        description="Maximum characters for the generated post (tweet is 280).",
    )

    audit: bool | None = Field(
        default=None,
        description=(
            "Enable auditing of the generated answer by a second LLM call. "
            "If None, uses env RAG_AUDIT_DEFAULT."
        ),
    )

    audit_rewrite: bool | None = Field(
        default=None,
        description=(
            "If true, and the auditor provides a fixed_answer, replace answer with it. "
            "If None, uses env RAG_AUDIT_REWRITE_DEFAULT."
        ),
    )

    audit_model: str | None = Field(
        default=None,
        description=(
            "Optional override model name for auditing (defaults to RAG_AUDIT_MODEL / OLLAMA_CHAT_MODEL)."
        ),
    )


class ChunkOut(BaseModel):
    id: str | None = None
    text: str
    distance: float | None = None
    metadata: dict = Field(default_factory=dict)


class AuditResult(BaseModel):
    model: str | None = None
    passed: bool = Field(..., description="True if the answer is supported by the provided context.")
    score: int = Field(..., ge=0, le=100, description="0-100 quality score of support & faithfulness.")
    confidence: Literal["low", "medium", "high"] | None = None
    issues: list[str] = Field(default_factory=list)
    fixed_answer: str | None = None
    original_answer: str | None = None
    raw: str | None = None


class QueryResponse(BaseModel):
    answer: str
    links: list[str] | None = None
    context: list[str] | None = None
    chunks: list[ChunkOut] | None = None
    removed_urls: list[str] | None = None
    audit: AuditResult | None = None


class StatusResponse(BaseModel):
    docs_dir: str
    json_files: int
    chunks_in_store: int
    files: list[str]


class ReindexResponse(BaseModel):
    documents: int
    chunks: int
    files: int


# -------------------------------------------------------------------
# Ollama chat wrapper
# -------------------------------------------------------------------


def _get_ollama_chat_model() -> str:
    return os.getenv("OLLAMA_CHAT_MODEL", "llama3.1")


def _get_ollama_base_url() -> str:
    return os.getenv("OLLAMA_BASE_URL", "http://ollama:11434").rstrip("/")

def _get_ollama_chat_timeout() -> int:
    """
    Seconds for requests timeout to Ollama /api/chat.
    """
    raw = os.getenv("OLLAMA_CHAT_TIMEOUT", "300")
    try:
        v = int(raw)
        return v if v > 0 else 300
    except Exception:
        return 300

def _call_ollama_chat(*, question: str, system_prompt: str, user_prompt: str) -> str:
    """Call Ollama's /api/chat endpoint."""
    base_url = _get_ollama_base_url()
    model = _get_ollama_chat_model()

    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "stream": False,
    }

    try:
        timeout_s = _get_ollama_chat_timeout()
        resp = _session.post(f"{base_url}/api/chat", json=payload, timeout=timeout_s)
        resp.raise_for_status()
        data = resp.json()

        message = data.get("message") or {}
        content = message.get("content")
        if not isinstance(content, str):
            raise RuntimeError("Ollama chat response missing 'message.content'")
    except Exception as e:
        logger.exception("Ollama chat failed: %s", e)
        raise RuntimeError(f"Ollama chat failed: {e}") from e

    return content


def _truthy_env(name: str, default: str = "false") -> bool:
    raw = os.getenv(name, default).strip().lower()
    return raw in {"1", "true", "yes", "y", "on"}


def _get_audit_default_enabled() -> bool:
    return _truthy_env("RAG_AUDIT_DEFAULT", "false")


def _get_audit_default_rewrite() -> bool:
    return _truthy_env("RAG_AUDIT_REWRITE_DEFAULT", "false")


def _get_ollama_audit_model() -> str:
    return os.getenv("RAG_AUDIT_MODEL") or _get_ollama_chat_model()


def _call_ollama_chat_with_model(*, model: str, system_prompt: str, user_prompt: str) -> str:
    """Call Ollama's /api/chat endpoint with an explicit model override."""
    base_url = _get_ollama_base_url()

    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "stream": False,
    }

    try:
        timeout_s = _get_ollama_chat_timeout()
        resp = _session.post(f"{base_url}/api/chat", json=payload, timeout=timeout_s)
        resp.raise_for_status()
        data = resp.json()
        message = data.get("message")
        if not isinstance(message, dict):
            raise RuntimeError("Ollama chat response missing 'message'")
        content = message.get("content")
        if not isinstance(content, str):
            raise RuntimeError("Ollama chat response missing 'message.content'")
        return content
    except Exception as e:
        logger.exception("Ollama audit chat failed: %s", e)
        raise RuntimeError(f"Ollama audit chat failed: {e}") from e


def _strip_code_fences(text: str) -> str:
    t = text.strip()
    if t.startswith("```"):
        t = re.sub(r"^```[a-zA-Z0-9_-]*\s*", "", t)
        t = re.sub(r"\s*```\s*$", "", t)
    return t.strip()


def _parse_first_json_object(text: str) -> dict | None:
    candidate = _strip_code_fences(text)
    try:
        obj = json.loads(candidate)
        if isinstance(obj, dict):
            return obj
    except Exception:
        pass

    # Fallback: find the first {...} block that parses as JSON.
    for m in re.finditer(r"\{.*?\}", candidate, flags=re.DOTALL):
        try:
            obj = json.loads(m.group(0))
            if isinstance(obj, dict):
                return obj
        except Exception:
            continue
    return None


def _build_audit_prompts(
    *,
    question: str,
    answer: str,
    rag_context: list[str],
    live_weather: str | None,
    allowed_urls: list[str],
    output_style: str,
    max_chars: int,
) -> tuple[str, str]:
    system_prompt = (
        "You are a strict auditor for a Retrieval-Augmented Generation (RAG) assistant.\n"
        "You will be given: QUESTION, ANSWER, and CONTEXT.\n"
        "Your job: judge whether the ANSWER is fully supported by the CONTEXT.\n"
        "Rules:\n"
        "- Use ONLY the provided CONTEXT; do NOT rely on outside knowledge.\n"
        "- If a claim is not clearly supported, mark it as an issue.\n"
        "- If the answer contains URLs not listed in ALLOWED_URLS, mark it as an issue.\n"
        "- Output MUST be a single JSON object and nothing else.\n"
        "\n"
        "JSON schema (keys must exist):\n"
        "{\n"
        "  \"passed\": boolean,\n"
        "  \"score\": integer 0-100,\n"
        "  \"confidence\": \"low\"|\"medium\"|\"high\",\n"
        "  \"issues\": array of strings,\n"
        "  \"fixed_answer\": string|null\n"
        "}\n"
        "\n"
        "If passed=false, provide fixed_answer that removes unsupported claims and stays within max_chars. "
        "Keep the same output_style as the original answer."
    )

    ctx_parts: list[str] = []
    for i, c in enumerate(rag_context, start=1):
        if not isinstance(c, str):
            continue
        c = c.strip()
        if not c:
            continue
        # Cap each context block to avoid runaway tokens
        if len(c) > 1500:
            c = c[:1500] + "…"
        ctx_parts.append(f"[{i}] {c}")

    user_prompt = (
        f"OUTPUT_STYLE: {output_style}\n"
        f"MAX_CHARS: {max_chars}\n"
        f"ALLOWED_URLS: {allowed_urls}\n"
        f"QUESTION:\n{question}\n\n"
        f"ANSWER:\n{answer}\n\n"
        f"CONTEXT:\n" + "\n\n".join(ctx_parts) + "\n\n"
        + (f"LIVE_WEATHER:\n{live_weather}\n\n" if live_weather else "")
    )

    return system_prompt, user_prompt


def _run_answer_audit(
    *,
    question: str,
    answer: str,
    rag_context: list[str],
    live_weather: str | None,
    allowed_urls: list[str],
    output_style: str,
    max_chars: int,
    audit_model: str,
    include_raw: bool,
) -> AuditResult:
    system_prompt, user_prompt = _build_audit_prompts(
        question=question,
        answer=answer,
        rag_context=rag_context,
        live_weather=live_weather,
        allowed_urls=allowed_urls,
        output_style=output_style,
        max_chars=max_chars,
    )

    raw = _call_ollama_chat_with_model(
        model=audit_model,
        system_prompt=system_prompt,
        user_prompt=user_prompt,
    )

    obj = _parse_first_json_object(raw)
    if not isinstance(obj, dict):
        return AuditResult(
            model=audit_model,
            passed=False,
            score=0,
            confidence="low",
            issues=["Audit model did not return valid JSON."],
            fixed_answer=None,
            raw=(raw if include_raw else None),
        )

    passed = bool(obj.get("passed", False))
    score = obj.get("score")
    try:
        score_i = int(score)
    except Exception:
        score_i = 0
    score_i = max(0, min(100, score_i))

    conf = obj.get("confidence")
    if conf not in {"low", "medium", "high"}:
        conf = "low"

    issues = obj.get("issues")
    if not isinstance(issues, list):
        issues_list: list[str] = []
    else:
        issues_list = [str(x) for x in issues if str(x).strip()]

    fixed_answer = obj.get("fixed_answer")
    if fixed_answer is not None and not isinstance(fixed_answer, str):
        fixed_answer = str(fixed_answer)

    return AuditResult(
        model=audit_model,
        passed=passed,
        score=score_i,
        confidence=conf,
        issues=issues_list,
        fixed_answer=fixed_answer,
        raw=(raw if include_raw else None),
    )




def _chunk_id_from_metadata(meta: dict) -> str | None:
    doc_id = meta.get("doc_id")
    idx = meta.get("chunk_index")
    if idx is None:
        idx = meta.get("index")
    if isinstance(doc_id, str) and doc_id and idx is not None:
        return f"{doc_id}:{idx}"
    return None


# -------------------------------------------------------------------
# Tweet-bot helpers (output only; no citations/sources in the text)
# -------------------------------------------------------------------


def _get_bot_name() -> str:
    return os.getenv("WEATHER_BOT_NAME", "YokoWeather")


def _get_bot_hashtags() -> str:
    # Space-separated or comma-separated accepted; we pass through to the model.
    return os.getenv("HASHTAGS", "#Yokosuka #MiuraPeninsula #Kanagawa")


def _clean_single_line(text: str) -> str:
    return " ".join(text.replace("\n", " ").replace("\r", " ").replace("\t", " ").split()).strip()


def _strip_wrapping_quotes(text: str) -> str:
    s = text.strip()
    pairs = [
        ('"', '"'),
        ("'", "'"),
        ("“", "”"),
        ("‘", "’"),
        ("`", "`"),
    ]
    for a, b in pairs:
        if s.startswith(a) and s.endswith(b) and len(s) >= 2:
            s = s[1:-1].strip()
    return s


def _enforce_max_chars(text: str, max_words: int) -> str:
    if max_words <= 0 or len(text) <= max_words:
        return text
    cut = text[: max_words - 1]
    if " " in cut:
        cut = cut.rsplit(" ", 1)[0]
    return cut.rstrip() + "…"


# -------------------------------------------------------------------
# URL safety: only allow URLs that appear in retrieved context (or explicit allowlists)
# -------------------------------------------------------------------

# NOTE: the double-quote inside the character class must be escaped for Python string syntax.
_URL_RE = re.compile(r"https?://[^\s<>()\[\]\"']+")
_MD_LINK_RE = re.compile(r"\[([^\]]+)\]\((https?://[^)\s]+)\)")

_TRAILING_PUNCT_RE = re.compile(r"[\]\)\}\>,\.;:!\?\"']+$")


def _normalize_url(url: str) -> str:
    """Normalize URLs for comparison (strip common trailing punctuation)."""
    u = url.strip()
    # Remove common trailing punctuation that appears in prose.
    u = _TRAILING_PUNCT_RE.sub("", u)
    return u


def _extract_urls_from_text(text: str) -> set[str]:
    return { _normalize_url(m.group(0)) for m in _URL_RE.finditer(text or "") }


def _get_extra_allowed_urls() -> set[str]:
    raw = os.getenv("RAG_EXTRA_ALLOWED_URLS", "")
    if not raw.strip():
        return set()
    parts = re.split(r"[\n,]+", raw)
    return {_normalize_url(p) for p in parts if p.strip()}


def _get_allowlist_regexes() -> list[re.Pattern[str]]:
    raw = os.getenv("RAG_URL_ALLOWLIST_REGEXES", "")
    if not raw.strip():
        return []
    patterns: list[re.Pattern[str]] = []
    for p in re.split(r"[\n,]+", raw):
        p = p.strip()
        if not p:
            continue
        try:
            patterns.append(re.compile(p))
        except re.error:
            logger.warning("Invalid URL allowlist regex ignored: %s", p)
    return patterns


def _is_allowed_url(url: str, *, allowed_urls: set[str], allowlist_regexes: list[re.Pattern[str]]) -> bool:
    u = _normalize_url(url)
    if u in allowed_urls:
        return True
    for rx in allowlist_regexes:
        if rx.search(u):
            return True
    return False

_INCOMPLETE_SCHEME_RE = re.compile(r"https?://(?=[\s\)\]\}\>,\.;:!\?\"']|$)")
_EMPTY_PARENS_RE = re.compile(r"\(\s*\)")

def _filter_answer_urls(
    answer: str,
    *,
    allowed_urls: set[str],
    allowlist_regexes: list[re.Pattern[str]],
) -> tuple[str, list[str]]:
    """Remove/harden URLs that are not allowed.

    - Markdown links: keep the label, drop the URL if not allowed.
    - Raw URLs: remove if not allowed.
    """
    removed: list[str] = []

    def _md_repl(m: re.Match[str]) -> str:
        label = m.group(1)
        url = _normalize_url(m.group(2))
        if _is_allowed_url(url, allowed_urls=allowed_urls, allowlist_regexes=allowlist_regexes):
            return f"[{label}]({url})"
        removed.append(url)
        return label  # drop the link, keep text

    out = _MD_LINK_RE.sub(_md_repl, answer or "")

    def _raw_repl(m: re.Match[str]) -> str:
        url = _normalize_url(m.group(0))
        if _is_allowed_url(url, allowed_urls=allowed_urls, allowlist_regexes=allowlist_regexes):
            return url
        removed.append(url)
        return ""

    out = _URL_RE.sub(_raw_repl, out)
    out = _INCOMPLETE_SCHEME_RE.sub("", out)     # "https://)", "https://"
    out = _EMPTY_PARENS_RE.sub("", out)          # "( )" , "()"

    # Cleanup: collapse extra spaces created by URL removal.
    out = re.sub(r"\s{2,}", " ", out).strip()
    removed_dedup = sorted({u for u in removed if u})
    return out, removed_dedup


def _collect_allowed_urls(context_texts: list[str], live_extra: str | None) -> set[str]:
    urls: set[str] = set()
    for t in context_texts:
        urls |= _extract_urls_from_text(t)
    if live_extra:
        urls |= _extract_urls_from_text(live_extra)
    urls |= _get_extra_allowed_urls()
    return {u for u in urls if u}


def _augment_prompts_with_url_policy(
    system_prompt: str,
    user_prompt: str,
    *,
    allowed_urls: set[str],
) -> tuple[str, str]:
    policy = (
        "\nURL POLICY:\n"
        "- Do NOT invent or guess URLs.\n"
        "- Only include URLs that appear in the 'Allowed URLs' list (verbatim).\n"
        "- If the Allowed URLs list is empty, do not include any URLs.\n"
    )
    sys2 = (system_prompt or "") + policy

    # Keep the list short to avoid bloating prompts.
    max_urls = int(os.getenv("RAG_MAX_ALLOWED_URLS_IN_PROMPT", "25") or "25")
    allowed_list = sorted(allowed_urls)[: max_urls if max_urls > 0 else 25]
    if allowed_list:
        urls_block = "\nAllowed URLs (copy/paste only):\n" + "\n".join(f"- {u}" for u in allowed_list) + "\n"
    else:
        urls_block = "\nAllowed URLs (copy/paste only):\n- (none)\n"

    usr2 = (user_prompt or "") + urls_block
    return sys2, usr2


def _first_nonempty_line(text: str) -> str:
    for line in (text or "").splitlines():
        s = line.strip()
        if s:
            return s
    return ""


def _extract_label_value(text: str, label: str) -> str | None:
    m = re.search(rf"(?im)^\s*{re.escape(label)}\s*:\s*(.+?)\s*$", text or "")
    if not m:
        return None
    v = m.group(1).strip()
    return v if v else None


def _links_from_meta(meta: dict[str, Any]) -> list[str]:
    def _clean_list(val: Any, *, split_commas: bool = False) -> list[str]:
        if val is None:
            return []
        if isinstance(val, list):
            return [str(x).strip() for x in val if isinstance(x, (str, int, float)) and str(x).strip()]
        if isinstance(val, str):
            s = val.strip()
            if not s:
                return []
            if split_commas and "," in s:
                return [p.strip() for p in s.split(",") if p.strip()]
            return [s]
        if isinstance(val, (int, float)):
            return [str(val)]
        return []

    links: list[str] = []
    for k in ("links", "link", "url", "permalink", "href", "source_url", "sourceUrl"):
        links.extend(_clean_list((meta or {}).get(k), split_commas=True))

    links = [u.strip() for u in links if isinstance(u, str) and u.strip()]
    links = [u for u in links if u.startswith("http://") or u.startswith("https://")]
    # normalize + dedupe while preserving order
    out: list[str] = []
    seen: set[str] = set()
    for u in links:
        nu = _normalize_url(u)
        if nu and nu not in seen:
            out.append(nu)
            seen.add(nu)
    return out


def _select_required_context(
    chunks: list[RAGChunk],
    *,
    fallback_links: list[str] | None = None,
) -> tuple[str | None, str | None]:
    """
    Pick a (mention, url) pair to force into the tweet:
    - Prefer a chunk that has BOTH a clear mention (TITLE/PLACE) and at least one link.
    - Otherwise, prefer a chunk that has a clear mention.
    - As a last resort, fall back to the first non-empty line of the top chunk.
    """
    fallback_links = fallback_links or []

    best_mention: str | None = None
    best_links: list[str] = []

    for c in chunks or []:
        meta = c.metadata if isinstance(c.metadata, dict) else {}

        mention = (
            (str(meta.get("title")).strip() if isinstance(meta.get("title"), str) else "")
            or (str(meta.get("place")).strip() if isinstance(meta.get("place"), str) else "")
            or (_extract_label_value(c.text, "TITLE") or "")
            or (_extract_label_value(c.text, "PLACE") or "")
        ).strip() or None

        links = _links_from_meta(meta)

        if mention and links:
            return mention, links[0]

        if best_mention is None and mention:
            best_mention = mention
            best_links = links

    if best_mention:
        return best_mention, (best_links[0] if best_links else (fallback_links[0] if fallback_links else None))

    # no TITLE/PLACE detected — still return something to anchor the tweet
    if chunks:
        fallback_mention = _first_nonempty_line(chunks[0].text)[:48].strip() or None
        return fallback_mention, (fallback_links[0] if fallback_links else None)

    return None, (fallback_links[0] if fallback_links else None)


def _project_context_for_tweet(ctx: str, *, max_len: int = 420) -> str:
    """
    Convert verbose chunk text into a compact, single-line summary for tweet writing.
    This helps the model reliably pick a spot/event and associated link.
    """
    if not isinstance(ctx, str) or not ctx.strip():
        return ""

    title = _extract_label_value(ctx, "TITLE")
    place = _extract_label_value(ctx, "PLACE")
    dt = _extract_label_value(ctx, "DATETIME")

    urls = sorted(_extract_urls_from_text(ctx))
    url = urls[0] if urls else None

    base = (ctx.split("----", 1)[0] if "----" in ctx else ctx).strip()
    base = re.sub(r"\s+", " ", base)
    if len(base) > 180:
        base = base[:177].rstrip() + "…"

    parts: list[str] = []
    if title:
        parts.append(f"TITLE: {title}")
    if place:
        parts.append(f"PLACE: {place}")
    if dt:
        parts.append(f"DATETIME: {dt}")
    if url:
        parts.append(f"URL: {url}")
    if base:
        parts.append(f"SNIPPET: {base}")

    out = " | ".join(parts).strip()
    if len(out) > max_len:
        out = out[: max_len - 1].rstrip() + "…"
    return out


def _enforce_tweet_contract(
    answer: str,
    *,
    max_chars: int,
    required_mention: str | None,
    required_url: str | None,
) -> str:
    """
    Guarantee:
    - mention appears verbatim (prefixed at start)
    - required_url appears exactly once at the end (if provided)
    - total length <= max_chars, preserving the URL when trimming
    """
    text = _strip_wrapping_quotes(_clean_single_line(answer or ""))
    mention = (required_mention or "").strip()
    url = _normalize_url(required_url) if isinstance(required_url, str) and required_url.strip() else ""

    # Ensure mention is at the start exactly once
    if mention:
        if not text.startswith(mention):
            if mention in text:
                text = text.replace(mention, "", 1).strip()
            text = f"{mention} — {text}".strip() if text else mention

    # Ensure URL is at the end exactly once
    if url:
        # remove existing occurrences
        text = re.sub(re.escape(url) + r"(?=\s|$)", "", text).strip()
        text = re.sub(r"\s{2,}", " ", text).strip()
        if text:
            text = f"{text} {url}".strip()
        else:
            text = url

    # Enforce length while preserving URL suffix
    if max_chars > 0 and len(text) > max_chars:
        suffix = f" {url}" if (url and text.endswith(url) and len(text) > len(url)) else (url if (url and text == url) else "")
        base = text[:-len(suffix)].rstrip() if suffix else text
        base_limit = max_chars - len(suffix)
        if base_limit <= 0:
            # can't fit base; return truncated suffix
            return (suffix or text)[:max_chars].rstrip()

        base = _enforce_max_chars(base, base_limit)
        text = (base + suffix).strip()
        if len(text) > max_chars:
            text = text[:max_chars].rstrip()

    return text


def _finalize_answer(
    answer: str,
    *,
    output_style: str,
    max_chars: int,
    required_mention: str | None,
    required_url: str | None,
) -> str:
    if output_style == "tweet_bot":
        return _enforce_tweet_contract(
            answer,
            max_chars=max_chars,
            required_mention=required_mention,
            required_url=required_url,
        )
    return _enforce_max_chars(answer, max_chars)


def _build_chat_prompts(
    *,
    question: str,
    rag_context: list[str],
    live_weather: str | None,
    output_style: str,
    max_words: int,
    place_hint: str | None,
    request_datetime: str | None = None,
    required_context_mention: str | None = None,
    required_context_url: str | None = None,
) -> tuple[str, str]:
    """
    Build (system, user) prompts.

    NOTE: Avoid Python's implicit string concatenation + conditional-expression footguns.
    Always build prompt parts explicitly to ensure LIVE WEATHER / RAG CONTEXT are never dropped.
    """
    if output_style != "tweet_bot":
        system = "You answer using the given context."
        ctx = "\n\n".join(rag_context + ([live_weather] if live_weather else []))

        parts: list[str] = [
            "Use the context below aside from general knowledge to answer the question.",
            "",
            "Context:",
            ctx,
            "",
            "Question:",
            question,
        ]
        if request_datetime and request_datetime.strip():
            parts.insert(0, f"REQUEST_DATETIME (authoritative): {request_datetime.strip()}")
            parts.insert(1, "")

        user = "\n".join(parts).strip() + "\n"
        return system, user

    bot_name = _get_bot_name()
    hashtags = _get_bot_hashtags()
    place = place_hint or os.getenv("PLACE") or "your area"

    now_dt, tz_name_for_now, _src = _resolve_now_datetime(live_weather, request_datetime)
    now_block = _format_now_block(live_weather, request_datetime=request_datetime)

    system = (
        f"You are {bot_name}, a friendly English local story bot for {place} (locals, familes and tourists). "
        f"Write one tweet in English upto {max_words} characters. "
        "No markdown, no lists, no extra commentary, no quotes.\n"
        "Show only real existing URLs.\n"
        "\n"
        "CONTEXT REQUIREMENTS (NON-NEGOTIABLE):\n"
        "- You MUST mention REQUIRED_CONTEXT_MENTION (from the user prompt) verbatim.\n"
        "- If REQUIRED_CONTEXT_URL is provided (and appears in Allowed URLs), include it exactly once.\n"
        "\n"
        "TIME AWARENESS:\n"
        "- Treat NOW (in user prompt) as the current local datetime.\n"
        "- If NOW.source is 'request_datetime', it is authoritative even if LIVE WEATHER differs.\n"
        "- Do NOT recommend events that are already in the past relative to NOW.\n"
        "- If you use words like 'today', 'tomorrow', or 'this weekend', they must match NOW.\n"
        "\n"
        "STYLE:\n"
        "- Warm, upbeat, practical.\n"
        "- Use emojis.\n"
        f"- If you add hashtags, pick 1-3 from: {hashtags}.\n"
    )

    def _sample_context(ctx: list[str], seed_text: str, k: int = 8, pool: int = 18) -> list[str]:
        if not ctx:
            return []
        cand = ctx[: min(len(ctx), pool)]

        # Prefer recent context when DATETIME metadata exists
        tz = _safe_zoneinfo(tz_name_for_now)

        def _ctx_dt(txt: str) -> datetime | None:
            m = re.search(r"(?im)^\s*DATETIME\s*:\s*(.+?)\s*$", txt or "")
            if not m:
                return None
            return _parse_datetime_like(m.group(1), tz_name_for_now)

        dated: list[tuple[datetime, str]] = []
        undated: list[str] = []
        for c in cand:
            d = _ctx_dt(c)
            if d is None:
                undated.append(c)
            else:
                dated.append((d.astimezone(tz), c))

        dated.sort(key=lambda x: x[0], reverse=True)
        ordered = [c for _d, c in dated] + undated

        seed = int(hashlib.sha256(seed_text.encode("utf-8")).hexdigest()[:8], 16)
        rng = random.Random(seed)
        rng.shuffle(ordered)
        return ordered[: min(k, len(ordered))]

    sampled = _sample_context(rag_context, question, k=8, pool=18)

    # Make tweet-bot context compact (avoid huge multi-line chunks / META_JSON bloat).
    compacted = [_project_context_for_tweet(c) for c in sampled]
    rag_lines = "\n".join(f"- {c}" for c in compacted if c) if compacted else "- (none)"

    live_block = live_weather.strip() if isinstance(live_weather, str) and live_weather.strip() else "(not available)"

    parts: list[str] = [
        "NOW (for time reasoning):",
        now_block,
    ]
    if request_datetime and request_datetime.strip():
        parts.extend([f"REQUEST_DATETIME (authoritative): {request_datetime.strip()}", ""])

    parts.extend(
        [
            "LIVE WEATHER:",
            live_block,
            "",
            "RAG CONTEXT (compact):",
            rag_lines,
            "",
            "REQUIRED FIELDS:",
            f"REQUIRED_CONTEXT_MENTION: {(required_context_mention or '').strip() or '(not available)'}",
            f"REQUIRED_CONTEXT_URL: {(required_context_url or '').strip() or '(not available)'}",
            "",
            "TASK:",
            question,
            "",
            "Remember: output ONLY the tweet text.",
        ]
    )

    user = "\n".join(parts).strip() + "\n"
    return system, user


# -------------------------------------------------------------------
# Routes
# -------------------------------------------------------------------


@router.get("/status", response_model=StatusResponse)
def status() -> StatusResponse:
    docs_dir = os.getenv("RAG_DOCS_DIR") or os.getenv("DOCS_DIR", "/data/json")
    file_paths = rag_store.list_json_files(docs_dir)
    file_names = [os.path.basename(p) for p in file_paths][:50]

    return StatusResponse(
        docs_dir=docs_dir,
        json_files=len(file_paths),
        chunks_in_store=rag_store.get_collection_count(),
        files=file_names,
    )


@router.post("/ingest", response_model=IngestResponse)
def ingest_rag(request: IngestRequest) -> IngestResponse:
    """(Optional) Ingest raw texts (kept for backwards-compat / testing).

    In production, prefer indexing from JSON files via /rag/reindex or startup auto-index.
    """
    docs = [d.strip() for d in request.documents if d and d.strip()]

    if not docs:
        raise HTTPException(
            status_code=HTTPStatus.BAD_REQUEST,
            detail="No documents provided.",
        )

    successes = 0
    last_error: Exception | None = None

    for text in docs:
        try:
            rag_store.add_document(text)
            successes += 1
        except Exception as exc:
            logger.exception("Failed to ingest document", exc_info=exc)
            last_error = exc

    if successes == 0 and last_error is not None:
        raise HTTPException(
            status_code=HTTPStatus.BAD_GATEWAY,
            detail=f"Document ingestion failed: {last_error}",
        )

    return IngestResponse(ingested=successes)


@router.post("/reindex", response_model=ReindexResponse)
def reindex() -> ReindexResponse:
    """Clear and rebuild the vector DB from JSON files in DOCS_DIR."""
    docs_dir = os.getenv("RAG_DOCS_DIR") or os.getenv("DOCS_DIR", "/data/json")
    enabled = _truthy(os.getenv("RAG_REINDEX_ENABLED", "true"))
    if not enabled:
        raise HTTPException(
            status_code=HTTPStatus.FORBIDDEN,
            detail="Reindex is disabled by configuration.",
        )

    try:
        stats = rag_store.rebuild_from_json_dir(docs_dir)
        return ReindexResponse(**stats)
    except Exception as exc:
        logger.exception("Reindex failed", exc_info=exc)
        raise HTTPException(
            status_code=HTTPStatus.BAD_GATEWAY,
            detail=str(exc),
        ) from exc


def _enrich_context_text_with_links(text: str, meta: dict[str, Any]) -> tuple[str, set[str]]:
    """Attach doc-level metadata (especially links) to every chunk's context.

    Why: chunking can separate the URL/title/time/place/tag from the relevant content chunk.
    By re-attaching metadata, the LLM can always see the full document context and URL allow-listing works.
    """

    def _clean_list(val: Any, *, split_commas: bool = False) -> list[str]:
        if val is None:
            return []
        if isinstance(val, list):
            return [str(x).strip() for x in val if isinstance(x, (str, int, float)) and str(x).strip()]
        if isinstance(val, str):
            s = val.strip()
            if not s:
                return []
            if split_commas and "," in s:
                return [p.strip() for p in s.split(",") if p.strip()]
            return [s]
        if isinstance(val, (int, float)):
            return [str(val)]
        return []

    # Collect links from metadata
    links: list[str] = []
    for k in ("links", "link", "url", "permalink", "href", "source_url", "sourceUrl"):
        links.extend(_clean_list((meta or {}).get(k), split_commas=True))
    links = [u for u in links if isinstance(u, str) and u and ("http://" in u or "https://" in u)]
    links = list(dict.fromkeys(links))

    # Collect tags (supports new `tag` and legacy `tags`)
    tags: list[str] = []
    for k in ("tag", "tags"):
        tags.extend(_clean_list((meta or {}).get(k), split_commas=True))
    tags = list(dict.fromkeys([t for t in tags if t]))

    # Build metadata block (keep it compact but complete)
    meta_lines: list[str] = []
    for key, label in (("title", "TITLE"), ("datetime", "DATETIME"), ("place", "PLACE")):
        v = (meta or {}).get(key)
        if isinstance(v, str) and v.strip():
            if not re.search(rf"^\s*{label}\s*:", text, flags=re.IGNORECASE | re.MULTILINE):
                meta_lines.append(f"{label}: {v.strip()}")

    if tags and not re.search(r"^\s*TAG\s*:", text, flags=re.IGNORECASE | re.MULTILINE):
        meta_lines.append("TAG: " + ", ".join(tags))

    if links:
        missing = [u for u in links if u not in text]
        if missing:
            meta_lines.append("LINK:")
            meta_lines.extend(missing)

    # NOTE: 全metadataをJSONとしてコンテクストへ流し込む（＝data.jsonの“コンテクスト全部”がRAGに渡る）
    meta_json_obj = dict(meta or {})
    try:
        meta_json = json.dumps(meta_json_obj, ensure_ascii=False, default=str)
    except Exception:
        meta_json = str(meta_json_obj)
    if meta_json and meta_json != "{}":
        meta_lines.append("META_JSON: " + meta_json)

    enriched = text.rstrip()
    if meta_lines:
        enriched = enriched + "\n\n----\n" + "\n".join(meta_lines)

    all_links = set(links) | set(_extract_urls_from_text(enriched))
    return enriched, all_links



@router.post("/query", response_model=QueryResponse, response_model_exclude_none=True)
def query_rag(payload: QueryRequest, http_request: Request) -> QueryResponse:
    """Run a full RAG cycle: retrieve similar chunks and ask the chat model."""
    try:
        tweet_pool = int(os.getenv("RAG_TWEET_CONTEXT_POOL", "18"))
        raw_k = max(payload.top_k * 4, tweet_pool * 4) if payload.output_style == "tweet_bot" else payload.top_k

        chunks: list[RAGChunk] = rag_store.query_similar_chunks(
            payload.question,
            top_k=raw_k,
        )

        if payload.output_style == "tweet_bot":
            seen = set()
            diversified = []
            for c in sorted(chunks, key=lambda x: x.distance):
                meta = c.metadata if isinstance(c.metadata, dict) else {}
                key = meta.get("file") or meta.get("doc_id") or c.text[:40]
                if key in seen:
                    continue
                diversified.append(c)
                seen.add(key)
                if len(diversified) >= tweet_pool:
                    break
            chunks = diversified or chunks[:tweet_pool]
    except Exception as exc:
        logger.exception("Vector search failed", exc_info=exc)
        raise HTTPException(
            status_code=HTTPStatus.BAD_GATEWAY,
            detail=f"Vector search failed: {exc}",
        ) from exc

    if not chunks:
        raise HTTPException(
            status_code=HTTPStatus.NOT_FOUND,
            detail="No relevant context found for the given question.",
        )

    context_texts: list[str] = []
    doc_links: set[str] = set()
    for c in chunks:
        meta = c.metadata if isinstance(c.metadata, dict) else {}
        enriched, links = _enrich_context_text_with_links(c.text, meta)
        context_texts.append(enriched)
        doc_links |= links

    # Compat: accept `context` / `user_context` as aliases for `extra_context`
    extra_ctx: str | None = payload.extra_context or payload.context or payload.user_context

    def _looks_like_weather_json(s: str) -> bool:
        try:
            obj = json.loads(s)
        except Exception:
            return False
        if not isinstance(obj, dict):
            return False
        cur = obj.get("current")
        return (
            isinstance(cur, dict)
            and isinstance(cur.get("time"), str)
            and bool(cur.get("time"))
        )

    live_extra: str | None = None
    user_extra: str | None = None
    if isinstance(extra_ctx, str) and extra_ctx.strip():
        if _looks_like_weather_json(extra_ctx):
            live_extra = extra_ctx
        else:
            user_extra = extra_ctx

    if (live_extra is None or not live_extra.strip()):
        try:
            live_extra = get_live_weather_context(
                http_request=http_request,
                session=_session,
            )
        except Exception as exc:
            logger.exception("Live weather fetch failed", exc_info=exc)
            raise HTTPException(
                status_code=HTTPStatus.SERVICE_UNAVAILABLE,
                detail=f"Live weather fetch failed: {exc}",
            ) from exc


    place_hint = http_request.query_params.get("place") or os.getenv("PLACE")

    # If caller provided non-weather extra context, keep it, but don't poison LIVE WEATHER.
    if user_extra:
        context_texts = context_texts + [f"EXTRA CONTEXT:\n{user_extra}"]

    # normalize request links
    req_links: list[str] = []
    for raw in (payload.links or []):
        if not isinstance(raw, str):
            continue
        u = _normalize_url(raw.strip())
        if u and (u.startswith("http://") or u.startswith("https://")):
            req_links.append(u)
        if len(req_links) >= 50:
            break

    allowed_urls = _collect_allowed_urls(context_texts, live_extra) | set(req_links)



    required_mention, required_url_candidate = _select_required_context(
        chunks,
        fallback_links=sorted(doc_links),
    )
    required_url = _normalize_url(required_url_candidate) if required_url_candidate else None
    if required_url and required_url not in allowed_urls:
        required_url = None

    system_prompt, user_prompt = _build_chat_prompts(
        question=payload.question,
        rag_context=context_texts,
        live_weather=live_extra,
        output_style=payload.output_style,
        max_words=payload.max_words,
        place_hint=place_hint,
        request_datetime=payload.datetime,
        required_context_mention=required_mention,
        required_context_url=required_url,
    )

    system_prompt, user_prompt = _augment_prompts_with_url_policy(
        system_prompt,
        user_prompt,
        allowed_urls=allowed_urls,
    )

    try:
        answer = _call_ollama_chat(
            question=payload.question,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
        )
    except Exception as exc:
        logger.exception("Ollama chat failed", exc_info=exc)
        raise HTTPException(
            status_code=HTTPStatus.BAD_GATEWAY,
            detail=str(exc),
        ) from exc

    answer = _strip_wrapping_quotes(_clean_single_line(answer))
    answer, removed_urls = _filter_answer_urls(
        answer,
        allowed_urls=allowed_urls,
        allowlist_regexes=_get_allowlist_regexes(),
    )
    answer = _finalize_answer(
        answer,
        output_style=payload.output_style,
        max_chars=payload.max_words,
        required_mention=required_mention,
        required_url=required_url,
    )

    # Optional: audit the answer with a second LLM call.
    audit_enabled = payload.audit if payload.audit is not None else _get_audit_default_enabled()
    audit_rewrite = payload.audit_rewrite if payload.audit_rewrite is not None else _get_audit_default_rewrite()
    audit_result: AuditResult | None = None
    if audit_enabled:
        audit_model = payload.audit_model or _get_ollama_audit_model()

        # Keep audit context bounded: reuse what we already sent to the writer model.
        audit_context = context_texts[:20]

        try:

            # keep auditor seeing the same request metadata (optional)
            audit_question = payload.question
            if payload.datetime:
                audit_question = f"REQUEST_DATETIME: {payload.datetime}\n" + audit_question
            if req_links:
                audit_question = (
                    "REQUEST_LINKS:\n" + "\n".join(f"- {u}" for u in req_links[:10]) + "\n\n" + audit_question
                )

            audit_result = _run_answer_audit(
                question=audit_question,
                answer=answer,
                rag_context=audit_context,
                live_weather=live_extra,
                allowed_urls=allowed_urls,
                output_style=payload.output_style,
                max_chars=payload.max_words,
                audit_model=audit_model,
                include_raw=bool(payload.include_debug),
            )
        except Exception as exc:
            # Don't fail the main request if auditing fails; surface the failure via audit_result.
            audit_result = AuditResult(
                model=audit_model,
                passed=False,
                score=0,
                confidence="low",
                issues=[f"Audit failed: {exc}"],
                fixed_answer=None,
                raw=(str(exc) if payload.include_debug else None),
            )

        if audit_rewrite and audit_result.fixed_answer:
            audit_result.original_answer = answer
            answer = _strip_wrapping_quotes(_clean_single_line(audit_result.fixed_answer))
            answer, removed_urls_2 = _filter_answer_urls(
                answer,
                allowed_urls=allowed_urls,
                allowlist_regexes=_get_allowlist_regexes(),
            )
            if removed_urls_2:
                removed_urls = (removed_urls or []) + removed_urls_2
            answer = _finalize_answer(
                answer,
                output_style=payload.output_style,
                max_chars=payload.max_words,
                required_mention=required_mention,
                required_url=required_url,
            )

    debug_context: list[str] | None = None
    debug_chunks: list[ChunkOut] | None = None

    if payload.include_debug:
        debug_context = list(context_texts)
        if live_extra and live_extra.strip():
            debug_context.append(f"[Live context]\n{live_extra.strip()}")

        chunk_out: list[ChunkOut] = []
        for c in chunks:
            meta = c.metadata if isinstance(c.metadata, dict) else {}
            chunk_out.append(
                ChunkOut(
                    id=_chunk_id_from_metadata(meta),
                    text=c.text,
                    distance=c.distance,
                    metadata=meta,
                )
            )
        debug_chunks = chunk_out

    # links for UI/debug: request links first, then doc_links
    links_out: list[str] = []
    seen = set()
    for u in req_links:
        if u not in seen:
            links_out.append(u)
            seen.add(u)
    for u in (sorted(doc_links) if doc_links else []):
        if u not in seen:
            links_out.append(u)
            seen.add(u)

    return QueryResponse(
        answer=answer,
        links=(links_out if links_out else None),
        context=debug_context,
        chunks=debug_chunks,
        removed_urls=(removed_urls if payload.include_debug and removed_urls else None),
        audit=audit_result,
    )
```

<a id="app-schemas.py"></a>
### 7. `app/schemas.py`
- Size: 697 bytes | LOC: 36 | SLOC: 22 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: ddc42df41d4e | Py: funcs=0                       classes=7                       complexity≈1

#### Brief
from pydantic import BaseModel, ConfigDict, EmailStr, Field

#### Auto Summary
Python module with 0 functions and 7 classes.

#### Content

```python
from pydantic import BaseModel, ConfigDict, EmailStr, Field


class ItemCreate(BaseModel):
    title: str = Field(..., min_length=1, max_length=120)


class ItemUpdate(BaseModel):
    title: str = Field(..., min_length=1, max_length=120)


class ItemOut(BaseModel):
    id: int
    title: str
    model_config = ConfigDict(from_attributes=True)


class UserCreate(BaseModel):
    email: EmailStr
    password: str = Field(..., min_length=6)


class UserOut(BaseModel):
    id: int
    email: EmailStr
    model_config = ConfigDict(from_attributes=True)


class SignIn(BaseModel):
    email: EmailStr
    password: str


class Token(BaseModel):
    access_token: str
    token_type: str = "bearer"
```

<a id="app-security.py"></a>
### 8. `app/security.py`
- Size: 860 bytes | LOC: 33 | SLOC: 22 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 32be6ea91102 | Py: funcs=4                       classes=0                       complexity≈3

#### Brief
import datetime as dt
import os

#### Auto Summary
Python module with 4 functions and 0 classes.

#### Content

```python
import datetime as dt
import os

from jose import jwt
from passlib.context import CryptContext

SECRET_KEY = os.getenv("AUTH_SECRET", "dev-secret")
ALGO = "HS256"
EXPIRE_MIN = int(os.getenv("AUTH_EXPIRE_MINUTES", "60"))

pwd_ctx = CryptContext(schemes=["bcrypt"], deprecated="auto")


def hash_password(pw: str) -> str:
    return pwd_ctx.hash(pw)


def verify_password(pw: str, hashed: str) -> bool:
    return pwd_ctx.verify(pw, hashed)


def create_access_token(sub: str) -> str:
    now = dt.datetime.now(dt.UTC)
    payload = {"sub": sub, "iat": now, "exp": now + dt.timedelta(minutes=EXPIRE_MIN)}
    return jwt.encode(payload, SECRET_KEY, algorithm=ALGO)


def decode_token(token: str) -> str | None:
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGO])
        return payload.get("sub")
    except Exception:
        return None
```

<a id="app-weather_service.py"></a>
### 9. `app/weather_service.py`
- Size: 10126 bytes | LOC: 302 | SLOC: 238 | TODOs: 0 | Modified: 2026-01-04 13:01:33 | SHA1: e5c47b5ccb8d | Py: funcs=13                       classes=3                       complexity≈80

#### Brief
from __future__ import annotations

#### Auto Summary
Python module with 13 functions and 1 classes.

#### Content

```python
from __future__ import annotations

import ipaddress
import os
from dataclasses import dataclass
from datetime import datetime
from typing import Any

import requests
from fastapi import Request


class WeatherError(RuntimeError):
    """Raised when live weather context cannot be produced."""


@dataclass(frozen=True)
class Location:
    latitude: float
    longitude: float
    timezone: str | None = None
    place: str | None = None
    source: str = "unknown"


@dataclass(frozen=True)
class WeatherSnapshot:
    observed_at: str  # ISO string from API
    temperature_c: float | None
    apparent_temperature_c: float | None
    humidity_pct: float | None
    precipitation_mm: float | None
    wind_speed_kmh: float | None
    wind_direction_deg: float | None
    weather_code: int | None


_WEATHER_CODE = {
    0: "clear sky",
    1: "mainly clear",
    2: "partly cloudy",
    3: "overcast",
    45: "fog",
    48: "depositing rime fog",
    51: "light drizzle",
    53: "moderate drizzle",
    55: "dense drizzle",
    56: "light freezing drizzle",
    57: "dense freezing drizzle",
    61: "slight rain",
    63: "moderate rain",
    65: "heavy rain",
    66: "light freezing rain",
    67: "heavy freezing rain",
    71: "slight snow fall",
    73: "moderate snow fall",
    75: "heavy snow fall",
    77: "snow grains",
    80: "slight rain showers",
    81: "moderate rain showers",
    82: "violent rain showers",
    85: "slight snow showers",
    86: "heavy snow showers",
    95: "thunderstorm",
    96: "thunderstorm with slight hail",
    99: "thunderstorm with heavy hail",
}


def _truthy(value: str | None) -> bool:
    if value is None:
        return False
    return value.strip().lower() in {"1", "true", "yes", "y", "on"}


def _parse_float(value: str | None) -> float | None:
    if value is None:
        return None
    try:
        return float(value)
    except ValueError:
        return None


def _is_public_ip(ip: str) -> bool:
    try:
        addr = ipaddress.ip_address(ip)
    except ValueError:
        return False
    return not (addr.is_private or addr.is_loopback or addr.is_link_local or addr.is_multicast or addr.is_reserved)


def _extract_client_ip(req: Request) -> str | None:
    # Respect a common reverse-proxy header. If multiple, take the left-most.
    xff = req.headers.get("x-forwarded-for") or req.headers.get("X-Forwarded-For")
    if xff:
        return xff.split(",")[0].strip()
    if req.client:
        return req.client.host
    return None


def _location_from_request_overrides(req: Request) -> Location | None:
    # 1) query params (useful for local dev): /rag/query?lat=...&lon=...
    lat = _parse_float(req.query_params.get("lat"))
    lon = _parse_float(req.query_params.get("lon"))

    # 2) headers (optional): X-Weather-Lat / X-Weather-Lon
    if lat is None:
        lat = _parse_float(req.headers.get("X-Weather-Lat") or req.headers.get("x-weather-lat"))
    if lon is None:
        lon = _parse_float(req.headers.get("X-Weather-Lon") or req.headers.get("x-weather-lon"))

    if lat is not None and lon is not None:
        tz = req.query_params.get("tz") or req.headers.get("X-Weather-Tz")
        place = req.query_params.get("place") or req.headers.get("X-Weather-Place")
        return Location(latitude=lat, longitude=lon, timezone=tz, place=place, source="request")

    return None


def _location_from_env() -> Location | None:
    lat = _parse_float(os.getenv("LAT"))
    lon = _parse_float(os.getenv("LON"))
    if lat is None or lon is None:
        return None
    return Location(
        latitude=lat,
        longitude=lon,
        timezone=os.getenv("TZ_NAME"),
        place=os.getenv("PLACE"),
        source="env",
    )


def _lookup_location_by_ip(ip: str, session: requests.Session, timeout_s: float) -> Location | None:
    # Try a few public, no-key endpoints. Each may rate-limit.
    candidates = [
        ("ipapi", f"https://ipapi.co/{ip}/json/"),
        ("ipinfo", f"https://ipinfo.io/{ip}/json"),
        ("ip-api", f"http://ip-api.com/json/{ip}?fields=status,message,lat,lon,city,regionName,country,timezone"),
    ]

    for name, url in candidates:
        try:
            r = session.get(url, timeout=timeout_s)
            r.raise_for_status()
            data: dict[str, Any] = r.json()
        except Exception:
            continue

        # Normalize the 3 providers.
        lat = data.get("latitude") or data.get("lat")
        lon = data.get("longitude") or data.get("lon")
        tz = data.get("timezone")
        city = data.get("city")
        region = data.get("region") or data.get("regionName")
        country = data.get("country_name") or data.get("country")

        if isinstance(lat, str):
            lat = _parse_float(lat)
        if isinstance(lon, str):
            lon = _parse_float(lon)

        if isinstance(lat, (int, float)) and isinstance(lon, (int, float)):
            place_parts = [p for p in [city, region, country] if isinstance(p, str) and p.strip()]
            place = ", ".join(place_parts) if place_parts else None
            return Location(latitude=float(lat), longitude=float(lon), timezone=tz if isinstance(tz, str) else None, place=place, source=f"ip:{name}")

    return None


def get_location(req: Request, session: requests.Session, timeout_s: float = 5.0) -> Location:
    # Request overrides first (nice for local dev).
    override = _location_from_request_overrides(req)
    if override:
        return override

    # Then env vars (nice for local dev + GitHub Actions).
    env_loc = _location_from_env()
    if env_loc:
        return env_loc

    # Finally IP lookup (works in real deployments; often not in local Docker).
    ip = _extract_client_ip(req)
    if not ip:
        raise WeatherError("Could not determine client IP for geolocation. Provide lat/lon via env or request params.")
    if not _is_public_ip(ip):
        raise WeatherError(
            f"Client IP '{ip}' is not a public IP (likely local/private). "
            "Provide lat/lon via env (LAT/LON) or request params (?lat=&lon=)."
        )

    loc = _lookup_location_by_ip(ip, session=session, timeout_s=timeout_s)
    if not loc:
        raise WeatherError("IP geolocation failed. Provide lat/lon via env or request params.")
    return loc


def fetch_current_weather(location: Location, session: requests.Session, timeout_s: float = 8.0) -> WeatherSnapshot:
    url = "https://api.open-meteo.com/v1/forecast"
    params = {
        "latitude": location.latitude,
        "longitude": location.longitude,
        "timezone": "auto" if not location.timezone else location.timezone,
        "current": ",".join(
            [
                "temperature_2m",
                "relative_humidity_2m",
                "apparent_temperature",
                "precipitation",
                "weather_code",
                "wind_speed_10m",
                "wind_direction_10m",
            ]
        ),
    }

    try:
        r = session.get(url, params=params, timeout=timeout_s)
        r.raise_for_status()
        data: dict[str, Any] = r.json()
    except Exception as exc:
        raise WeatherError(f"Weather API request failed: {exc}") from exc

    current = data.get("current") or {}
    return WeatherSnapshot(
        observed_at=str(current.get("time") or ""),
        temperature_c=_as_float(current.get("temperature_2m")),
        apparent_temperature_c=_as_float(current.get("apparent_temperature")),
        humidity_pct=_as_float(current.get("relative_humidity_2m")),
        precipitation_mm=_as_float(current.get("precipitation")),
        wind_speed_kmh=_as_float(current.get("wind_speed_10m")),
        wind_direction_deg=_as_float(current.get("wind_direction_10m")),
        weather_code=_as_int(current.get("weather_code")),
    )


def _as_float(v: Any) -> float | None:
    if v is None:
        return None
    if isinstance(v, (int, float)):
        return float(v)
    if isinstance(v, str):
        return _parse_float(v)
    return None


def _as_int(v: Any) -> int | None:
    if v is None:
        return None
    if isinstance(v, int):
        return v
    if isinstance(v, float):
        return int(v)
    if isinstance(v, str):
        try:
            return int(float(v))
        except ValueError:
            return None
    return None


def format_live_weather_context(location: Location, weather: WeatherSnapshot) -> str:
    desc = _WEATHER_CODE.get(weather.weather_code, f"weather_code={weather.weather_code}" if weather.weather_code is not None else "unknown")
    where = location.place or f"{location.latitude:.4f},{location.longitude:.4f}"
    parts: list[str] = [
        f"Location: {where} (source={location.source})",
    ]
    if weather.observed_at:
        parts.append(f"Observed at: {weather.observed_at}")
    parts.append(f"Condition: {desc}")

    if weather.temperature_c is not None:
        t = f"{weather.temperature_c:.1f}°C"
        if weather.apparent_temperature_c is not None:
            t += f" (feels {weather.apparent_temperature_c:.1f}°C)"
        parts.append(f"Temperature: {t}")

    if weather.humidity_pct is not None:
        parts.append(f"Humidity: {weather.humidity_pct:.0f}%")

    if weather.precipitation_mm is not None:
        parts.append(f"Precipitation: {weather.precipitation_mm:.1f} mm")

    if weather.wind_speed_kmh is not None:
        wind = f"{weather.wind_speed_kmh:.1f} km/h"
        if weather.wind_direction_deg is not None:
            wind += f" @ {weather.wind_direction_deg:.0f}°"
        parts.append(f"Wind: {wind}")

    # A reminder for the LLM: this is ephemeral and should not be stored.
    parts.append("Note: This is live, short-lived context; do NOT store it in the vector DB.")

    return "\n".join(parts)


def get_live_weather_context(*, http_request: Request, session: requests.Session) -> str:
    """Return a short, human-readable block that can be appended to the RAG prompt."""
    loc = get_location(http_request, session=session)
    snap = fetch_current_weather(loc, session=session)
    return format_live_weather_context(loc, snap)
```

<a id="Dockerfile"></a>
### 10. `Dockerfile`
- Size: 183 bytes | LOC: 10 | SLOC: 7 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 574aaa461398

#### Brief
FROM python:3.12-slim

#### Auto Summary
FROM python:3.12-slim

#### Content

```dockerfile
FROM python:3.12-slim

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

WORKDIR /root
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

WORKDIR /app
```

<a id="Dockerfile.test"></a>
### 11. `Dockerfile.test`
- Size: 187 bytes | LOC: 6 | SLOC: 4 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: bd7fa3c1240c

#### Brief
FROM python:3.12-slim

#### Auto Summary
FROM python:3.12-slim

#### Content

```
FROM python:3.12-slim

WORKDIR /root
COPY requirements.txt requirements.test.txt pyproject.toml pytest.ini ./
RUN pip install --no-cache-dir -r requirements.txt -r requirements.test.txt
```

<a id="pyproject.toml"></a>
### 12. `pyproject.toml`
- Size: 235 bytes | LOC: 12 | SLOC: 10 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: d8ea70d2cd22

#### Brief
[tool.ruff]
target-version = "py312"

#### Auto Summary
[tool.ruff]

#### Content

```toml
[tool.ruff]
target-version = "py312"
line-length = 100
extend-exclude = ["migrations", "build", "dist", "node_modules", ".venv"]

[tool.ruff.lint]
select = ["E", "F", "W", "I", "UP", "B", "PL", "RUF"]
ignore = [
  "E203", 
  "B008"
]
```

<a id="pytest.ini"></a>
### 13. `pytest.ini`
- Size: 254 bytes | LOC: 11 | SLOC: 11 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: a36bbb3f96e4

#### Brief
[pytest]
testpaths = /tests

#### Auto Summary
[pytest]

#### Content

```ini
[pytest]
testpaths = /tests
python_files = test_*.py
addopts =
    -q
    --cov=/app
    --cov-branch
    --cov-report=term-missing:skip-covered
    --cov-report=xml:/reports/ci/coverage.xml
    --cov-report=html:/reports/ci/html
    --cov-fail-under=80
```

<a id="requirements.test.txt"></a>
### 14. `requirements.test.txt`
- Size: 101 bytes | LOC: 6 | SLOC: 6 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 439a63e65157

#### Brief
-r requirements.txt
pytest==8.4.2

#### Auto Summary
-r requirements.txt

#### Content

```
-r requirements.txt
pytest==8.4.2
pytest-asyncio==1.2.0
pytest-cov==7.0.0
httpx==0.28.1
ruff==0.14.8
```

<a id="requirements.txt"></a>
### 15. `requirements.txt`
- Size: 291 bytes | LOC: 15 | SLOC: 15 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 5fb62d8b396c

#### Brief
fastapi==0.124.0
uvicorn[standard]==0.38.0

#### Auto Summary
fastapi==0.124.0

#### Content

```
fastapi==0.124.0
uvicorn[standard]==0.38.0
SQLAlchemy==2.0.44
psycopg[binary]==3.2.13
python-dotenv==1.2.1
python-jose==3.5.0
passlib==1.7.4
email-validator==2.3.0
bcrypt==4.3.0
alembic==1.13.3
psycopg2-binary==2.9.10
pydantic==2.9.2
pydantic-settings==2.6.1
chromadb==1.3.5
requests==2.32.5
```

<a id="tests-conftest.py"></a>
### 16. `tests/conftest.py`
- Size: 1271 bytes | LOC: 46 | SLOC: 35 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 1002e87420e8 | Py: funcs=5                       classes=0                       complexity≈3

#### Brief
from collections.abc import Iterator
from http import HTTPStatus

#### Auto Summary
Python module with 5 functions and 0 classes.

#### Content

```python
from collections.abc import Iterator
from http import HTTPStatus

import pytest
from database import engine
from fastapi.testclient import TestClient
from main import app
from models import Base
from sqlalchemy import text


@pytest.fixture(scope="session", autouse=True)
def _create_tables_once() -> Iterator[None]:
    Base.metadata.create_all(bind=engine)
    yield
    Base.metadata.drop_all(bind=engine)


@pytest.fixture(autouse=True)
def _clean_db() -> Iterator[None]:
    with engine.begin() as conn:
        conn.execute(text("DELETE FROM items"))
        conn.execute(text("DELETE FROM users"))
    yield


@pytest.fixture()
def client() -> Iterator[TestClient]:
    with TestClient(app) as c:
        yield c


@pytest.fixture()
def auth_token(client: TestClient) -> str:
    email = "tester@example.com"
    pw = "secretpw"
    r = client.post("/auth/signup", json={"email": email, "password": pw})
    assert r.status_code in (HTTPStatus.CREATED, HTTPStatus.BAD_REQUEST)
    r = client.post("/auth/signin", json={"email": email, "password": pw})
    assert r.status_code == HTTPStatus.OK, r.text
    return r.json()["access_token"]


@pytest.fixture()
def auth_headers(auth_token: str) -> dict[str, str]:
    return {"Authorization": f"Bearer {auth_token}"}
```

<a id="tests-test_auth.py"></a>
### 17. `tests/test_auth.py`
- Size: 1960 bytes | LOC: 60 | SLOC: 45 | TODOs: 1 | Modified: 2025-12-30 23:13:23 | SHA1: 472ab7d97e4e | Py: funcs=7                       classes=0                       complexity≈4

#### Brief
from http import HTTPStatus

#### Auto Summary
Python module with 7 functions and 0 classes.

#### Content

```python
from http import HTTPStatus

from fastapi.testclient import TestClient


def test_signup_success(client: TestClient):
    r = client.post(
        "/auth/signup", json={"email": "u1@example.com", "password": "123456"}
    )
    assert r.status_code == HTTPStatus.CREATED, r.text
    data = r.json()
    assert data["email"] == "u1@example.com"
    assert "id" in data


def test_signup_short_password(client):
    r = client.post("/auth/signup", json={"email": "u2@example.com", "password": "123"})
    assert r.status_code == HTTPStatus.UNPROCESSABLE_ENTITY
    detail = r.json().get("detail", [])
    assert any(
        (d.get("loc")[-1] == "password") and ("string_too_short" in d.get("type", ""))
        for d in detail
    )


def test_signup_duplicate_email(client: TestClient):
    client.post("/auth/signup", json={"email": "u3@example.com", "password": "123456"})
    r = client.post(
        "/auth/signup", json={"email": "u3@example.com", "password": "abcdef"}
    )
    assert r.status_code == HTTPStatus.BAD_REQUEST
    assert "Email already registered" in r.text


def test_signin_ok(client: TestClient):
    client.post("/auth/signup", json={"email": "u4@example.com", "password": "abcdef"})
    r = client.post(
        "/auth/signin", json={"email": "u4@example.com", "password": "abcdef"}
    )
    assert r.status_code == HTTPStatus.OK
    token = r.json().get("access_token")
    assert token and isinstance(token, str)


def test_signin_invalid(client: TestClient):
    r = client.post(
        "/auth/signin", json={"email": "nope@example.com", "password": "xxxxx"}
    )
    assert r.status_code == HTTPStatus.UNAUTHORIZED


def test_me_requires_token(client: TestClient):
    r = client.get("/auth/me")
    assert r.status_code == HTTPStatus.UNAUTHORIZED


def test_me_ok(client: TestClient, auth_headers):
    r = client.get("/auth/me", headers=auth_headers)
    assert r.status_code == HTTPStatus.OK
    assert "email" in r.json()
```

<a id="tests-test_auth_extra.py"></a>
### 18. `tests/test_auth_extra.py`
- Size: 1704 bytes | LOC: 48 | SLOC: 31 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 284087709bcb | Py: funcs=4                       classes=0                       complexity≈3

#### Brief
from http import HTTPStatus

#### Auto Summary
Python module with 4 functions and 0 classes.

#### Content

```python
from http import HTTPStatus

import security
from fastapi.testclient import TestClient


# 1) double sign up -> 409 or 400
def test_signup_duplicate_email(client: TestClient):
    email = "dup@example.com"
    pw = "abcdef"  # min length
    r1 = client.post("/auth/signup", json={"email": email, "password": pw})
    assert r1.status_code in (HTTPStatus.OK, HTTPStatus.CREATED)

    r2 = client.post("/auth/signup", json={"email": email, "password": pw})
    assert r2.status_code in (HTTPStatus.BAD_REQUEST, HTTPStatus.CONFLICT)


# 2) password inconsistent -> 401
def test_signin_wrong_password(client: TestClient):
    email = "wrongpw@example.com"
    ok_pw = "abcdef"
    client.post("/auth/signup", json={"email": email, "password": ok_pw})

    r = client.post("/auth/signin", json={"email": email, "password": "not-correct"})
    assert r.status_code == HTTPStatus.UNAUTHORIZED


# 3) bad token /auth/me -> 401: JWT decode failure
def test_me_invalid_token(client: TestClient):
    r = client.get("/auth/me", headers={"Authorization": "Bearer not-a-token"})
    assert r.status_code == HTTPStatus.UNAUTHORIZED


# 4) expired token /auth/me -> 401: exp check
def test_me_expired_token(client: TestClient, monkeypatch):
    email = "expired@example.com"
    pw = "abcdef"
    client.post("/auth/signup", json={"email": email, "password": pw})

    original = security.EXPIRE_MIN
    monkeypatch.setattr(security, "EXPIRE_MIN", -1)
    try:
        token = security.create_access_token(email)
    finally:
        monkeypatch.setattr(security, "EXPIRE_MIN", original)

    r = client.get("/auth/me", headers={"Authorization": f"Bearer {token}"})
    assert r.status_code == HTTPStatus.UNAUTHORIZED
```

<a id="tests-test_chunk_text.py"></a>
### 19. `tests/test_chunk_text.py`
- Size: 717 bytes | LOC: 23 | SLOC: 16 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: f2a2aa4e1ce0 | Py: funcs=1                       classes=0                       complexity≈5

#### Brief
from __future__ import annotations

#### Auto Summary
Python module with 1 functions and 0 classes.

#### Content

```python
from __future__ import annotations

from rag_store import DocumentChunk, chunk_text

EXPECTED_CHUNK_COUNT = 3

def test_chunk_text_splits_long_text_and_sets_metadata() -> None:
    words = [f"word{i}" for i in range(25)]
    text = " ".join(words)

    chunks = chunk_text(text, max_tokens=10)

    assert len(chunks) == EXPECTED_CHUNK_COUNT

    for idx, chunk in enumerate(chunks):
        assert isinstance(chunk, DocumentChunk)
        assert chunk.text.strip()
        assert isinstance(chunk.metadata, dict)
        assert chunk.metadata.get("chunk_index") == idx or chunk.metadata.get("index") == idx

    total = len(chunks)
    for chunk in chunks:
        assert chunk.metadata.get("total_chunks") == total
```

<a id="tests-test_health.py"></a>
### 20. `tests/test_health.py`
- Size: 308 bytes | LOC: 12 | SLOC: 8 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 32f13e5fcb02 | Py: funcs=1                       classes=0                       complexity≈1

#### Brief
backend/tests/test_health.py

#### Auto Summary
Python module with 1 functions and 0 classes.

#### Content

```python
# backend/tests/test_health.py
from http import HTTPStatus

from fastapi.testclient import TestClient


def test_health_ok(client: TestClient):
    r = client.get("/health")
    assert r.status_code == HTTPStatus.OK
    body = r.json()
    assert body.get("status") == "ok"
    assert body.get("db") is True
```

<a id="tests-test_main_lifespan.py"></a>
### 21. `tests/test_main_lifespan.py`
- Size: 2820 bytes | LOC: 86 | SLOC: 65 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: c1ac2bcb1fe1 | Py: funcs=5                       classes=0                       complexity≈5

#### Brief
from __future__ import annotations

#### Auto Summary
Python module with 5 functions and 0 classes.

#### Content

```python
from __future__ import annotations

import asyncio

import main
import pytest


def test_truthy_variants() -> None:
    assert main._truthy("1")
    assert main._truthy("true")
    assert main._truthy("YES")
    assert not main._truthy("0")
    assert not main._truthy("")
    assert not main._truthy(None)


def test_lifespan_auto_index_ingest(monkeypatch: pytest.MonkeyPatch, tmp_path) -> None:
    called = {"create_all": 0, "ingest": None, "rebuild": None}

    monkeypatch.setattr(
        main.Base.metadata,
        "create_all",
        lambda bind=None: called.__setitem__("create_all", called["create_all"] + 1))
    monkeypatch.setattr(
        main.rag_store, "ingest_json_dir",
        lambda d: called.__setitem__("ingest", d))
    monkeypatch.setattr(
        main.rag_store, "rebuild_from_json_dir",
        lambda d: called.__setitem__("rebuild", d))
    monkeypatch.setenv("RAG_AUTO_INDEX", "true")
    monkeypatch.setenv("DOCS_DIR", str(tmp_path))
    monkeypatch.setenv("RAG_REBUILD_ON_START", "false")
    monkeypatch.setenv("RAG_INDEX_FAIL_FAST", "false")

    async def run():
        async with main.lifespan(main.app):
            pass

    asyncio.run(run())

    assert called["create_all"] == 1
    assert called["ingest"] == str(tmp_path)
    assert called["rebuild"] is None


def test_lifespan_auto_index_rebuild(monkeypatch: pytest.MonkeyPatch, tmp_path) -> None:
    called = {"rebuild": None}
    monkeypatch.setattr(main.Base.metadata, "create_all", lambda bind=None: None)
    monkeypatch.setattr(
        main.rag_store, "rebuild_from_json_dir",
        lambda d: called.__setitem__("rebuild", d))
    monkeypatch.setattr(main.rag_store, "ingest_json_dir", lambda d: None)
    monkeypatch.setenv("RAG_AUTO_INDEX", "true")
    monkeypatch.setenv("DOCS_DIR", str(tmp_path))
    monkeypatch.setenv("RAG_REBUILD_ON_START", "true")
    monkeypatch.setenv("RAG_INDEX_FAIL_FAST", "false")

    async def run():
        async with main.lifespan(main.app):
            pass

    asyncio.run(run())
    assert called["rebuild"] == str(tmp_path)


def test_lifespan_fail_fast_re_raises(monkeypatch: pytest.MonkeyPatch, tmp_path) -> None:
    monkeypatch.setattr(main.Base.metadata, "create_all", lambda bind=None: None)

    def boom(_d: str):
        raise RuntimeError("ingest failed")

    monkeypatch.setattr(main.rag_store, "ingest_json_dir", boom)
    monkeypatch.setattr(main.rag_store, "rebuild_from_json_dir", lambda d: None)

    monkeypatch.setenv("RAG_AUTO_INDEX", "true")
    monkeypatch.setenv("DOCS_DIR", str(tmp_path))
    monkeypatch.setenv("RAG_REBUILD_ON_START", "false")
    monkeypatch.setenv("RAG_INDEX_FAIL_FAST", "true")

    async def run():
        async with main.lifespan(main.app):
            pass

    with pytest.raises(RuntimeError):
        asyncio.run(run())
```

<a id="tests-test_rag_api.py"></a>
### 22. `tests/test_rag_api.py`
- Size: 9577 bytes | LOC: 274 | SLOC: 205 | TODOs: 0 | Modified: 2026-01-03 18:08:19 | SHA1: 1e1c59ddd413 | Py: funcs=27                       classes=0                       complexity≈3

#### Brief
from __future__ import annotations

#### Auto Summary
Python module with 27 functions and 0 classes.

#### Content

```python
from __future__ import annotations

from http import HTTPStatus

import pytest
import rag_store
import routers.rag as rag_router
from fastapi.testclient import TestClient
from rag_store import RAGChunk


def test_rag_query_strips_incomplete_scheme(client, monkeypatch):
    allowed = "https://example.com/allowed"
    chunk_text = f"Tourism info. Link: {allowed}"

    monkeypatch.setattr(
        rag_store,
        "query_similar_chunks",
        lambda query, top_k=3: [
            rag_store.RAGChunk(text=chunk_text, distance=0.1, metadata={"source": "test"})
        ],
    )

    def fake_call_ollama_chat(*, question: str, system_prompt: str, user_prompt: str) -> str:
        return f"Nice spot! (https://) Official: {allowed}"

    monkeypatch.setattr(rag_router, "_call_ollama_chat", fake_call_ollama_chat)

    r = client.post("/rag/query", json={"question": "hello"})
    assert r.status_code == 200
    ans = r.json()["answer"]
    assert allowed in ans
    assert "(https://)" not in ans
    assert "https://" not in ans
    

def test_rag_ingest_success(client: TestClient, monkeypatch: pytest.MonkeyPatch) -> None:
    calls: list[str] = []

    def fake_add_document(text: str) -> None:
        calls.append(text)

    monkeypatch.setattr(rag_store, "add_document", fake_add_document)

    r = client.post("/rag/ingest", json={"documents": ["Doc1", "Doc2"]})

    assert r.status_code == HTTPStatus.OK, r.text
    assert r.json() == {"ingested": 2}
    assert calls == ["Doc1", "Doc2"]


def test_rag_ingest_empty_documents_returns_400(client: TestClient) -> None:
    r = client.post("/rag/ingest", json={"documents": []})
    assert r.status_code == HTTPStatus.BAD_REQUEST
    body = r.json()
    assert "documents" in body.get("detail", "").lower() or "empty" in body.get(
        "detail", ""
    ).lower()


def test_rag_ingest_all_fail_returns_502(
    client: TestClient, monkeypatch: pytest.MonkeyPatch
) -> None:
    def fake_add_document(_text: str) -> None:
        raise RuntimeError("test failure")

    monkeypatch.setattr(rag_store, "add_document", fake_add_document)

    r = client.post("/rag/ingest", json={"documents": ["x", "y"]})
    assert r.status_code == HTTPStatus.BAD_GATEWAY
    assert "failed" in r.json().get("detail", "").lower()


def test_rag_query_success(
    client: TestClient, monkeypatch: pytest.MonkeyPatch
) -> None:
    def fake_query_similar_chunks(question: str, top_k: int = 3) -> list[RAGChunk]:
        return [
            RAGChunk(
                text="Miura Peninsula is located in Kanagawa Prefecture.",
                distance=0.01,
                metadata={"source": "test"},
            )
        ]

    monkeypatch.setattr(rag_store, "query_similar_chunks", fake_query_similar_chunks)

    def fake_call_ollama_chat(*, question: str, system_prompt: str, user_prompt: str) -> str:
        assert "Miura Peninsula" in user_prompt
        return f"ANSWER to: {question}"

    monkeypatch.setattr(rag_router, "_call_ollama_chat", fake_call_ollama_chat)

    r = client.post("/rag/query", json={"question": "Where is Miura Peninsula?"})

    assert r.status_code == HTTPStatus.OK, r.text
    data = r.json()
    assert data["answer"].startswith("ANSWER to:")
    assert data["context"] == [
        "Miura Peninsula is located in Kanagawa Prefecture."
    ]


def test_rag_query_uses_extra_context(client: TestClient, monkeypatch: pytest.MonkeyPatch) -> None:
    def fake_query_similar_chunks(question: str, top_k: int = 3) -> list[RAGChunk]:
        return [
            RAGChunk(
                text="Yokosuka is a coastal city in Kanagawa Prefecture.",
                distance=0.02,
                metadata={"source": "test"},
            )
        ]

    monkeypatch.setattr(rag_store, "query_similar_chunks", fake_query_similar_chunks)

    def fake_call_ollama_chat(*, question: str, system_prompt: str, user_prompt: str) -> str:
        assert "Yokosuka is a coastal city" in context
        assert "[Live context]" in context
        assert "Current: 10°C" in context
        return "OK"

    monkeypatch.setattr(rag_router, "_call_ollama_chat", fake_call_ollama_chat)

    r = client.post(
        "/rag/query",
        json={
            "question": "Write a short weather tweet.",
            "top_k": 1,
            "extra_context": "[Weather] 2025-12-17 (Asia/Tokyo)\\n- Current: 10°C, Clear",
        },
    )

    assert r.status_code == HTTPStatus.OK, r.text
    data = r.json()
    assert data["answer"] == "OK"


def test_rag_query_no_context_returns_404(
    client: TestClient, monkeypatch: pytest.MonkeyPatch
) -> None:
    monkeypatch.setattr(rag_store, "query_similar_chunks", lambda q, top_k=3: [])

    r = client.post("/rag/query", json={"question": "hello?"})

    assert r.status_code == HTTPStatus.NOT_FOUND
    assert "No relevant context" in r.json().get("detail", "")


def test_rag_query_ollama_failure_returns_502(
    client: TestClient, monkeypatch: pytest.MonkeyPatch
) -> None:
    def fake_query_similar_chunks(question: str, top_k: int = 3) -> list[RAGChunk]:
        return [
            RAGChunk(
                text="Some stored chunk",
                distance=0.5,
                metadata={"source": "test"},
            )
        ]

    monkeypatch.setattr(rag_store, "query_similar_chunks", fake_query_similar_chunks)

    def fake_call_ollama_chat(*, question: str, system_prompt: str, user_prompt: str) -> str:
        raise RuntimeError("Ollama is down")

    monkeypatch.setattr(rag_router, "_call_ollama_chat", fake_call_ollama_chat)

    r = client.post("/rag/query", json={"question": "hello?"})

    assert r.status_code == HTTPStatus.BAD_GATEWAY
    assert "Ollama is down" in r.json().get("detail", "")

def test_rag_query_filters_unknown_urls(client, monkeypatch):
    # One URL appears in retrieved context => allowed.
    allowed = "https://example.com/allowed"
    chunk_text = f"Tourism info. Link: {allowed}"

    monkeypatch.setattr(
        rag_store,
        "query_similar_chunks",
        lambda query, top_k=3: [
            rag_store.RAGChunk(
                text=chunk_text,
                distance=0.1,
                metadata={"source": "test"},
            )
        ],
    )

    def fake_call_ollama_chat(*, question: str, system_prompt: str, user_prompt: str) -> str:
        # Model tries to include a URL that is NOT in the retrieved context
        return f"Use {allowed} and also https://example.com/NOT_ALLOWED"

    monkeypatch.setattr(rag_router, "_call_ollama_chat", fake_call_ollama_chat)

    r = client.post("/rag/query", json={"question": "hello"})
    assert r.status_code == 200

    data = r.json()
    assert allowed in data["answer"]
    assert "https://example.com/NOT_ALLOWED" not in data["answer"]


def test_rag_query_audit_returns_verdict(client, monkeypatch):
    allowed = "https://example.com/allowed"
    chunk_text = f"Tourism info. Link: {allowed}"

    def fake_query_similar_chunks(question: str, top_k: int):
        return [
            RAGChunk(
                text=chunk_text,
                distance=0.1,
                metadata={"doc_id": "doc", "chunk_index": 0},
            )
        ]

    monkeypatch.setattr(rag_store, "query_similar_chunks", fake_query_similar_chunks)

    def fake_call_ollama_chat(*, question: str, system_prompt: str, user_prompt: str) -> str:
        return f"Use {allowed}."

    def fake_call_ollama_chat_with_model(*, model: str, system_prompt: str, user_prompt: str) -> str:
        return (
            '{"passed": true, "score": 95, "confidence": "high", "issues": [], "fixed_answer": null}'
        )

    monkeypatch.setattr(rag_router, "_call_ollama_chat", fake_call_ollama_chat)
    monkeypatch.setattr(rag_router, "_call_ollama_chat_with_model", fake_call_ollama_chat_with_model)

    r = client.post("/rag/query", json={"question": "hello", "audit": True})
    assert r.status_code == 200
    data = r.json()
    assert "audit" in data
    assert data["audit"]["passed"] is True
    assert data["audit"]["score"] == 95


def test_rag_query_audit_can_rewrite_answer(client, monkeypatch):
    allowed = "https://example.com/allowed"
    chunk_text = f"Tourism info. Link: {allowed}"

    def fake_query_similar_chunks(question: str, top_k: int):
        return [
            RAGChunk(
                text=chunk_text,
                distance=0.1,
                metadata={"doc_id": "doc", "chunk_index": 0},
            )
        ]

    monkeypatch.setattr(rag_store, "query_similar_chunks", fake_query_similar_chunks)

    def fake_call_ollama_chat(*, question: str, system_prompt: str, user_prompt: str) -> str:
        return f"Unsupported claim. Read more: https://example.com/NOT_ALLOWED"

    def fake_call_ollama_chat_with_model(*, model: str, system_prompt: str, user_prompt: str) -> str:
        return (
            '{"passed": false, "score": 10, "confidence": "low", "issues": ["unsupported"], '
            '"fixed_answer": "Use https://example.com/allowed."}'
        )

    monkeypatch.setattr(rag_router, "_call_ollama_chat", fake_call_ollama_chat)
    monkeypatch.setattr(rag_router, "_call_ollama_chat_with_model", fake_call_ollama_chat_with_model)

    r = client.post(
        "/rag/query",
        json={"question": "hello", "audit": True, "audit_rewrite": True, "include_debug": True},
    )
    assert r.status_code == 200
    data = r.json()
    assert data["audit"]["passed"] is False
    assert allowed in data["answer"]
    assert "NOT_ALLOWED" not in data["answer"]
    assert "original_answer" in data["audit"]
```

<a id="tests-test_rag_embed_and_chunk_jp.py"></a>
### 23. `tests/test_rag_embed_and_chunk_jp.py`
- Size: 4740 bytes | LOC: 145 | SLOC: 95 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 7c9650234886 | Py: funcs=16                       classes=1                       complexity≈10

#### Brief
from __future__ import annotations

#### Auto Summary
Python module with 16 functions and 0 classes.

#### Content

```python
from __future__ import annotations

from typing import Any

import pytest
import rag_store
from rag_store import chunk_text


# 1) JP_SENT_SPLIT
def test_chunk_text_splits_japanese_sentences_and_sets_metadata() -> None:
    text = "三浦半島は神奈川県にあります。海がきれいです。山もあります。"

    # ★ max_tokens
    chunks = chunk_text(text, max_tokens=50)

    texts = [c.text.strip() for c in chunks]
    assert texts == [
        "三浦半島は神奈川県にあります。",
        "海がきれいです。",
        "山もあります。",
    ]

    assert [c.metadata["chunk_index"] for c in chunks] == [0, 1, 2]
    TOTAL_CHUNKS=3
    assert all(c.metadata["total_chunks"] == TOTAL_CHUNKS for c in chunks)

def test_chunk_text_splits_long_japanese_sentence_by_char_limit() -> None:
    # max_tokens
    text = "三浦半島は神奈川県にあります。海がきれいです。山もあります。"
    chunks = chunk_text(text, max_tokens=10)

    texts = [c.text.strip() for c in chunks]
    assert texts == [
        "三浦半島は神奈川県に",
        "あります。",
        "海がきれいです。",
        "山もあります。",
    ]
    assert [c.metadata["chunk_index"] for c in chunks] == [0, 1, 2, 3]
    TOTAL_CHUNKS=4
    assert all(c.metadata["total_chunks"] == TOTAL_CHUNKS for c in chunks)

def test__embed_with_ollama_raises_runtime_error_on_unexpected_payload(
    monkeypatch: pytest.MonkeyPatch,
) -> None:
    def fake_post(url: str, json: dict[str, Any], timeout: int) -> _DummyResponse:
        return _DummyResponse({"unexpected": "shape"})

    monkeypatch.setattr(rag_store.requests, "post", fake_post)

    # ValueError
    with pytest.raises(RuntimeError) as excinfo:
        rag_store._embed_with_ollama("dummy text")

    assert "Ollama embedding failed" in str(excinfo.value)
    assert isinstance(excinfo.value.__cause__, ValueError)
    assert "Unexpected embedding response format" in str(excinfo.value.__cause__)

# 2) embed_texts:  ->
def test_embed_texts_empty_list_returns_empty() -> None:
    assert rag_store.embed_texts([]) == []


# 3) embed_texts: -> cleaned_texts  return []
def test_embed_texts_only_blank_strings_returns_empty() -> None:
    vectors = rag_store.embed_texts(["   ", "", "\n\t"])
    assert vectors == []


# 4) embed_texts:  _embed_with_ollama
def test_embed_texts_raises_runtime_error_when_all_calls_fail(
    monkeypatch: pytest.MonkeyPatch,
) -> None:
    def always_fail(_text: str) -> list[float]:
        raise RuntimeError("boom")

    monkeypatch.setattr(rag_store, "_embed_with_ollama", always_fail)

    with pytest.raises(RuntimeError) as excinfo:
        rag_store.embed_texts(["hello", "world"])

    msg = str(excinfo.value)
    assert "Failed to embed any of the 2 text chunks" in msg


# 5) _embed_with_ollama:
class _DummyResponse:
    def __init__(self, payload: dict[str, Any]) -> None:
        self._payload = payload

    def raise_for_status(self) -> None:
        return None

    def json(self) -> dict[str, Any]:
        return self._payload


@pytest.mark.parametrize(
    "payload, expected",
    [
        # {"embeddings": [[...]]}
        ({"embeddings": [[0.1, 0.2, 0.3]]}, [0.1, 0.2, 0.3]),
        # {"embedding": [...]}
        ({"embedding": [1.0, 2.0]}, [1.0, 2.0]),
        # {"data": [{"embedding": [...]}]}
        ({"data": [{"embedding": [3.0]}]}, [3.0]),
    ],
)
def test__embed_with_ollama_accepts_multiple_response_shapes(
    monkeypatch: pytest.MonkeyPatch,
    payload: dict[str, Any],
    expected: list[float],
) -> None:
    def fake_post(url: str, json: dict[str, Any], timeout: int) -> _DummyResponse:
        return _DummyResponse(payload)

    monkeypatch.setattr(rag_store.requests, "post", fake_post)

    vec = rag_store._embed_with_ollama("dummy text")
    assert vec == expected


# 6) _embed_with_ollama:
def test__embed_with_ollama_raises_value_error_on_unexpected_payload(
    monkeypatch: pytest.MonkeyPatch,
) -> None:
    def fake_post(url: str, json: dict[str, Any], timeout: int) -> _DummyResponse:
        # embedding
        return _DummyResponse({"unexpected": "shape"})

    monkeypatch.setattr(rag_store.requests, "post", fake_post)


    with pytest.raises(RuntimeError) as excinfo:
        rag_store._embed_with_ollama("dummy text")

    assert isinstance(excinfo.value.__cause__, ValueError)
    assert "Unexpected embedding response format" in str(excinfo.value.__cause__)

    msg = str(excinfo.value)
    assert "Unexpected embedding response format" in msg

def test_extract_embedding_from_response_returns_none_on_unexpected_payload() -> None:
    assert rag_store._extract_embedding_from_response({"unexpected": "shape"}) is None
```

<a id="tests-test_rag_ollama_helpers.py"></a>
### 24. `tests/test_rag_ollama_helpers.py`
- Size: 2434 bytes | LOC: 79 | SLOC: 52 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: c45bfacec3df | Py: funcs=9                       classes=2                       complexity≈2

#### Brief
from __future__ import annotations

#### Auto Summary
Python module with 9 functions and 0 classes.

#### Content

```python
from __future__ import annotations

from typing import Any

import pytest
import routers.rag as rag_router


def test_get_ollama_chat_model_default(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.delenv("OLLAMA_CHAT_MODEL", raising=False)

    value = rag_router._get_ollama_chat_model()
    assert value == "llama3.1"


def test_get_ollama_chat_model_env_override(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setenv("OLLAMA_CHAT_MODEL", "custom-model")

    value = rag_router._get_ollama_chat_model()
    assert value == "custom-model"


def test_get_ollama_base_url_default(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.delenv("OLLAMA_BASE_URL", raising=False)

    value = rag_router._get_ollama_base_url()
    assert value == "http://ollama:11434"


def test_get_ollama_base_url_env_override_trims_slash(
    monkeypatch: pytest.MonkeyPatch,
) -> None:
    monkeypatch.setenv("OLLAMA_BASE_URL", "http://example.com:1234/")

    value = rag_router._get_ollama_base_url()
    assert value == "http://example.com:1234"


def test_call_ollama_chat_uses_session_and_parses_content(
    monkeypatch: pytest.MonkeyPatch,
) -> None:
    calls: list[dict[str, Any]] = []

    class DummyResponse:
        def __init__(self, payload: dict[str, Any]) -> None:
            self._payload = payload

        def raise_for_status(self) -> None:
            return None

        def json(self) -> dict[str, Any]:
            return self._payload

    class DummySession:
        def post(self, url: str, json: dict[str, Any], timeout: int) -> DummyResponse:
            calls.append({"url": url, "json": json, "timeout": timeout})
            return DummyResponse({"message": {"content": "dummy answer"}})

    monkeypatch.setenv("OLLAMA_BASE_URL", "http://ollama:11434/")
    monkeypatch.setenv("OLLAMA_CHAT_MODEL", "llama3.1")

    monkeypatch.setattr(rag_router, "_session", DummySession())

    result = rag_router._call_ollama_chat(
        question="What is Miura Peninsula?",
        context="Some context about Miura Peninsula.",
    )

    assert result == "dummy answer"

    assert calls, "DummySession.post should have been called at least once."
    call = calls[0]

    assert call["url"].endswith("/api/chat")

    payload = call["json"]
    assert payload["model"] == "llama3.1"
    assert payload.get("stream") is False
    assert any(m["role"] == "user" for m in payload.get("messages", []))
```

<a id="tests-test_rag_store.py"></a>
### 25. `tests/test_rag_store.py`
- Size: 3345 bytes | LOC: 113 | SLOC: 81 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 600d9effa463 | Py: funcs=10                       classes=1                       complexity≈8

#### Brief
from __future__ import annotations

#### Auto Summary
Python module with 10 functions and 0 classes.

#### Content

```python
from __future__ import annotations

from typing import Any

import pytest
import rag_store


class DummyCollection:
    """Very small in-memory stand-in for the Chroma collection."""

    def __init__(self) -> None:
        self.records: list[dict[str, Any]] = []

    def count(self) -> int:
        return len(self.records)

    def add(
        self,
        embeddings: list[list[float]],
        documents: list[str],
        metadatas: list[dict[str, Any]],
        ids: list[str],
    ) -> None:
        for emb, doc, meta, _id in zip(
            embeddings, documents, metadatas, ids, strict=False
        ):
            self.records.append(
                {
                    "id": _id,
                    "document": doc,
                    "metadata": meta,
                    "embedding": emb,
                }
            )

    def query(
        self,
        query_embeddings: list[list[float]],
        n_results: int,
        include: list[str] | None = None,
    ) -> dict[str, Any]:
        docs = [r["document"] for r in self.records][:n_results]
        metas = [r["metadata"] for r in self.records][:n_results]
        dists = [0.1 for _ in docs]

        return {
            "documents": [docs],
            "metadatas": [metas],
            "distances": [dists],
        }


def test_embed_texts_uses_ollama_wrapper(monkeypatch: pytest.MonkeyPatch) -> None:
    called: list[str] = []

    def fake_embed(text: str) -> list[float]:
        called.append(text)
        return [0.1, 0.2, 0.3]

    monkeypatch.setattr(rag_store, "_embed_with_ollama", fake_embed)

    vectors = rag_store.embed_texts(["hello", "world"])

    assert vectors == [[0.1, 0.2, 0.3], [0.1, 0.2, 0.3]]
    assert called == ["hello", "world"]


def test_add_document_stores_all_chunks(monkeypatch: pytest.MonkeyPatch) -> None:
    dummy_collection = DummyCollection()

    monkeypatch.setattr(rag_store, "_collection", dummy_collection)

    def fake_embed_texts(texts: list[str]) -> list[list[float]]:
        return [[float(i)] for i, _ in enumerate(texts)]

    monkeypatch.setattr(rag_store, "embed_texts", fake_embed_texts)

    text = "One sentence about Miura. Another one about Yokosuka."
    rag_store.add_document(text)

    assert dummy_collection.count() == len(dummy_collection.records) > 0

    first = dummy_collection.records[0]
    assert "document" in first
    assert "metadata" in first
    assert "chunk_index" in first["metadata"]


def test_query_similar_chunks_returns_rag_chunks(monkeypatch: pytest.MonkeyPatch) -> None:
    dummy_collection = DummyCollection()
    monkeypatch.setattr(rag_store, "_collection", dummy_collection)

    def fake_embed_texts(texts: list[str]) -> list[list[float]]:
        return [[0.0] for _ in texts]

    monkeypatch.setattr(rag_store, "embed_texts", fake_embed_texts)

    rag_store.add_document("First document about Miura.")
    rag_store.add_document("Second document about Yokosuka.")

    monkeypatch.setattr(rag_store, "embed_texts", fake_embed_texts)

    EXPECTED_TOP_K = 2

    chunks = rag_store.query_similar_chunks("Tell me about Yokosuka", top_k=EXPECTED_TOP_K)

    assert len(chunks) == EXPECTED_TOP_K
    c0 = chunks[0]
    assert hasattr(c0, "text")
    assert hasattr(c0, "distance")
    assert hasattr(c0, "metadata")
    assert isinstance(c0.metadata, dict)
```

<a id="tests-test_rag_store_collection_ops.py"></a>
### 26. `tests/test_rag_store_collection_ops.py`
- Size: 4224 bytes | LOC: 130 | SLOC: 91 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 02b14948976d | Py: funcs=20                       classes=4                       complexity≈6

#### Brief
from __future__ import annotations

#### Auto Summary
Python module with 20 functions and 0 classes.

#### Content

```python
from __future__ import annotations

from typing import Any

import pytest
import rag_store


class DummyCollectionUpsert:
    def __init__(self) -> None:
        self.deleted: list[dict[str, Any]] = []
        self.add_calls: list[dict[str, Any]] = []
        self.upsert_calls: list[dict[str, Any]] = []
        self._count = 0

    def count(self) -> int:
        return self._count

    def delete(self, **kwargs: Any) -> None:
        self.deleted.append(kwargs)

    def add(self, **kwargs: Any) -> None:
        self.add_calls.append(kwargs)
        self._count += len(kwargs.get("documents") or [])

    def upsert(self, **kwargs: Any) -> None:
        self.upsert_calls.append(kwargs)
        self._count = max(self._count, len(kwargs.get("documents") or []))


class DummyCollectionNoUpsert:
    def __init__(self) -> None:
        self.deleted: list[dict[str, Any]] = []
        self.add_calls: list[dict[str, Any]] = []
        self._count = 0

    def count(self) -> int:
        return self._count

    def delete(self, **kwargs: Any) -> None:
        self.deleted.append(kwargs)

    def add(self, **kwargs: Any) -> None:
        self.add_calls.append(kwargs)
        self._count += len(kwargs.get("documents") or [])

def test_get_collection_count_returns_count(monkeypatch: pytest.MonkeyPatch) -> None:
    COLLECTION_COUNT = 7
    dummy = DummyCollectionUpsert()
    dummy._count = COLLECTION_COUNT
    monkeypatch.setattr(rag_store, "_get_collection", lambda: dummy)
    assert rag_store.get_collection_count() == COLLECTION_COUNT


def test_get_collection_count_returns_zero_on_exception(monkeypatch: pytest.MonkeyPatch) -> None:
    class Boom:
        def count(self) -> int:
            raise RuntimeError("boom")

    monkeypatch.setattr(rag_store, "_get_collection", lambda: Boom())
    assert rag_store.get_collection_count() == 0


def test_delete_by_doc_id_calls_delete(monkeypatch: pytest.MonkeyPatch) -> None:
    dummy = DummyCollectionNoUpsert()
    monkeypatch.setattr(rag_store, "_get_collection", lambda: dummy)

    rag_store._delete_by_doc_id("doc-1")

    assert dummy.deleted
    assert dummy.deleted[0].get("where") == {"doc_id": "doc-1"}


def test_delete_by_doc_id_ignores_errors(monkeypatch: pytest.MonkeyPatch) -> None:
    class Boom:
        def delete(self, **_kwargs: Any) -> None:
            raise RuntimeError("boom")

    monkeypatch.setattr(rag_store, "_get_collection", lambda: Boom())
    rag_store._delete_by_doc_id("doc-1")  # 例外が外に出ないこと


def test_upsert_document_uses_upsert_when_available(monkeypatch: pytest.MonkeyPatch) -> None:
    dummy = DummyCollectionUpsert()
    monkeypatch.setattr(rag_store, "_get_collection", lambda: dummy)

    def fake_embed_texts(texts: list[str]) -> list[list[float]]:
        return [[float(i)] for i, _ in enumerate(texts)]

    monkeypatch.setattr(rag_store, "embed_texts", fake_embed_texts)

    n = rag_store.upsert_document(
        doc_id="doc-1",
        text="三浦半島は神奈川県にあります。海がきれいです。",
        metadata={"source": "test", "tags": ["miura", "sea"]},
    )

    assert n > 0
    assert len(dummy.upsert_calls) == 1
    assert dummy.add_calls == []
    assert dummy.deleted == []


def test_upsert_document_falls_back_to_delete_and_add(monkeypatch: pytest.MonkeyPatch) -> None:
    dummy = DummyCollectionNoUpsert()
    monkeypatch.setattr(rag_store, "_get_collection", lambda: dummy)

    def fake_embed_texts(texts: list[str]) -> list[list[float]]:
        return [[float(i)] for i, _ in enumerate(texts)]

    monkeypatch.setattr(rag_store, "embed_texts", fake_embed_texts)

    n = rag_store.upsert_document(
        doc_id="doc-1",
        text="三浦半島は神奈川県にあります。海がきれいです。",
        metadata={"source": "test"},
    )

    assert n > 0
    assert dummy.deleted
    assert dummy.add_calls

CHUNK_SIZE = 17

def test_get_chunk_size_env_parsing(monkeypatch: pytest.MonkeyPatch) -> None:
    monkeypatch.setenv("RAG_CHUNK_SIZE", CHUNK_SIZE)
    assert rag_store._get_chunk_size() == CHUNK_SIZE

    monkeypatch.setenv("RAG_CHUNK_SIZE", "nope")
    assert rag_store._get_chunk_size() == rag_store._DEFAULT_CHUNK_SIZE
```

<a id="tests-test_rag_store_config.py"></a>
### 27. `tests/test_rag_store_config.py`
- Size: 1654 bytes | LOC: 50 | SLOC: 33 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: be72d2ca6ceb | Py: funcs=8                       classes=0                       complexity≈2

#### Brief
import rag_store

#### Auto Summary
Python module with 8 functions and 0 classes.

#### Content

```python
import rag_store


def test_get_chroma_db_dir_default(monkeypatch):
    monkeypatch.delenv("CHROMA_DB_DIR", raising=False)
    value = rag_store._get_chroma_db_dir()
    assert value == "/chroma"


def test_get_chroma_db_dir_env(monkeypatch, tmp_path):
    monkeypatch.setenv("CHROMA_DB_DIR", str(tmp_path))
    value = rag_store._get_chroma_db_dir()
    assert value == str(tmp_path)


def test_get_chroma_collection_name_default(monkeypatch):
    monkeypatch.delenv("CHROMA_COLLECTION_NAME", raising=False)
    value = rag_store._get_chroma_collection_name()
    assert value == "documents"


def test_get_chroma_collection_name_env(monkeypatch):
    monkeypatch.setenv("CHROMA_COLLECTION_NAME", "my_collection")
    value = rag_store._get_chroma_collection_name()
    assert value == "my_collection"


def test_get_ollama_base_url_default(monkeypatch):
    monkeypatch.delenv("OLLAMA_BASE_URL", raising=False)
    value = rag_store._get_ollama_base_url()
    assert value == "http://ollama:11434"


def test_get_ollama_base_url_env(monkeypatch):
    monkeypatch.setenv("OLLAMA_BASE_URL", "http://example.com:1234")
    value = rag_store._get_ollama_base_url()
    assert value == "http://example.com:1234"


def test_get_embedding_model_default(monkeypatch):
    monkeypatch.delenv("EMBEDDING_MODEL", raising=False)
    value = rag_store._get_embedding_model()
    # just make sure it's not empty and matches the default
    assert value == "nomic-embed-text"


def test_get_embedding_model_env(monkeypatch):
    monkeypatch.setenv("EMBEDDING_MODEL", "my-embed-model")
    value = rag_store._get_embedding_model()
    assert value == "my-embed-model"
```

<a id="tests-test_rag_store_ingestion_json.py"></a>
### 28. `tests/test_rag_store_ingestion_json.py`
- Size: 3011 bytes | LOC: 92 | SLOC: 68 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 6fc3e9aef059 | Py: funcs=6                       classes=0                       complexity≈6

#### Brief
from __future__ import annotations

#### Auto Summary
Python module with 6 functions and 0 classes.

#### Content

```python
from __future__ import annotations

import json
from pathlib import Path

import pytest
import rag_store


def test_list_json_files_sorted(tmp_path: Path) -> None:
    (tmp_path / "b.json").write_text("[]", encoding="utf-8")
    (tmp_path / "a.json").write_text("[]", encoding="utf-8")
    (tmp_path / "ignore.txt").write_text("x", encoding="utf-8")

    got = rag_store.list_json_files(str(tmp_path))
    assert got == [str(tmp_path / "a.json"), str(tmp_path / "b.json")]


def test_escape_control_chars_makes_invalid_json_parseable() -> None:
    # JSON
    raw = '{"id":"1","text":"hello\nworld","metadata":{}}'
    fixed = rag_store._escape_control_chars_inside_json_strings(raw)
    data = json.loads(fixed)
    assert data["text"] == "hello\nworld"


def test_load_json_file_repairs_and_normalizes(tmp_path: Path) -> None:
    p = tmp_path / "x.json"
    # text -> _load_json_file
    p.write_text('{"id":"doc1","text":"hello\nworld","metadata":{"tags":["miura"]}}',
                 encoding="utf-8")

    docs = rag_store._load_json_file(str(p))
    assert len(docs) == 1
    d = docs[0]
    assert d["id"] == "doc1"
    assert d["text"] == "hello\nworld"
    assert d["file"] == "x.json"
    assert d["metadata"]["tags"] == ["miura"]
    assert d["source"].startswith("file://")


def test_ingest_json_dir_counts_documents_chunks_files(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path) -> None:
    (tmp_path / "a.json").write_text(
        json.dumps([{"id": "a1", "text": "aaa", "metadata": {"tags": ["miura"]}}],
                   ensure_ascii=False),
        encoding="utf-8",
    )
    (tmp_path / "b.json").write_text(
        json.dumps(
            [
                {"id": "b1", "text": "bbb"},
                {"id": "b2", "text": "ccc", "metadata": {"k": 1}},
            ],
            ensure_ascii=False,
        ),
        encoding="utf-8",
    )

    calls = []

    def fake_upsert(doc_id: str, text: str, *, source=None, metadata=None, max_tokens=None) -> int:
        calls.append((doc_id, text, source, metadata))
        return 2  # 1 doc 2 chunks

    monkeypatch.setattr(rag_store, "upsert_document", fake_upsert)

    stats = rag_store.ingest_json_dir(str(tmp_path))
    assert stats == {"documents": 3, "chunks": 6, "files": 2}

    # metadata  meta.setdefault("file", ...)
    assert any((m or {}).get("file") == "a.json" for _, _, _, m in calls)
    assert any((m or {}).get("file") == "b.json" for _, _, _, m in calls)


def test_rebuild_from_json_dir_calls_reset_then_ingest(
        monkeypatch: pytest.MonkeyPatch,
        tmp_path: Path) -> None:
    events = []

    monkeypatch.setattr(rag_store, "reset_collection", lambda: events.append("reset"))
    monkeypatch.setattr(
        rag_store,
        "ingest_json_dir",
        lambda d: (events.append(f"ingest:{d}") or {"documents": 0, "chunks": 0, "files": 0}),
    )

    rag_store.rebuild_from_json_dir(str(tmp_path))
    assert events[0] == "reset"
    assert events[1] == f"ingest:{tmp_path!s}"
```

<a id="tests-test_schemas_security_underscore.py"></a>
### 29. `tests/test_schemas_security_underscore.py`
- Size: 1456 bytes | LOC: 43 | SLOC: 27 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 4e1e35eea3fa | Py: funcs=5                       classes=0                       complexity≈8

#### Brief
from __future__ import annotations

#### Auto Summary
Python module with 5 functions and 0 classes.

#### Content

```python
from __future__ import annotations

import importlib

import pytest
from pydantic import ValidationError


def _import(name: str):
    try:
        return importlib.import_module(name)
    except ModuleNotFoundError:
        return None


# Prefer underscored modules if present, otherwise fall back.
schemas_ = _import("schemas_") or importlib.import_module("schemas")
security_ = _import("security_") or importlib.import_module("security")


def test_schemas_usercreate_valid_email() -> None:
    u = schemas_.UserCreate(email="a@example.com", password="pw1234")  # len==6
    assert u.email == "a@example.com"
    assert u.password == "pw1234"


def test_schemas_usercreate_invalid_email_raises() -> None:
    with pytest.raises(ValidationError) as ex:
        schemas_.UserCreate(email="not-an-email", password="pw1234")  # keep pw valid
    # ensure the error is about email (not password)
    assert any(err.get("loc", [])[-1] == "email" for err in ex.value.errors())


def test_security_hash_and_verify_password() -> None:
    hashed = security_.hash_password("pw1234")
    assert security_.verify_password("pw1234", hashed) is True
    assert security_.verify_password("wrongpw", hashed) is False


def test_security_token_roundtrip_and_invalid_token() -> None:
    token = security_.create_access_token("user@example.com")
    assert security_.decode_token(token) == "user@example.com"
    assert security_.decode_token("this.is.not.jwt") is None
```

<a id="tests-test_smoke.py"></a>
### 30. `tests/test_smoke.py`
- Size: 259 bytes | LOC: 12 | SLOC: 8 | TODOs: 0 | Modified: 2025-12-30 23:13:23 | SHA1: 26fa4f6654ef | Py: funcs=1                       classes=0                       complexity≈1

#### Brief
from http import HTTPStatus

#### Auto Summary
Python module with 1 functions and 0 classes.

#### Content

```python
from http import HTTPStatus

from fastapi.testclient import TestClient
from main import app

client = TestClient(app)


def test_openapi_available():
    r = client.get("/openapi.json")
    assert r.status_code == HTTPStatus.OK
    assert "paths" in r.json()
```
